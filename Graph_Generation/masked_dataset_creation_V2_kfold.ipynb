{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData, Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_networkx, subgraph, degree, from_networkx\n",
    "from torch.utils.data import Dataset, dataloader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.nn import GCNConv, summary, GraphSAGE\n",
    "from torch.nn import Sequential, Linear, ReLU, Dropout\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import community.community_louvain as community_louvain  # python-louvain\n",
    "\n",
    "# visual\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "#device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Work\\UQAM\\Doctorat\\Projets\\oignion_GNN\\cultures_GNN\\carotte\\Automne_2023\\Script\\graph_masking\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrot_df = pd.read_csv('../../Output/carrot_no_sensitive_data.csv', index_col=0)\n",
    "meteo_df = pd.read_csv('../../Output/combined_daily_meteo.csv', index_col=0)\n",
    "plant_distance_filepath = \"../../Output/field_distance.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(plant_distance_filepath) as file:\n",
    "    distance_txt_file = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FarmID\n",
      "Date\n",
      "Day_avg_Temp_C\n",
      "Day_max_Temp_C\n",
      "Day_min_Temp_C\n",
      "Day_less_5_Temp_C\n",
      "Day_less_13_Temp_C\n",
      "Day_more_30_Temp_C\n",
      "Day_more_35_Temp_C\n",
      "Day_15_25_Temp_C\n",
      "Day_18_25_Temp_C\n",
      "Day_18_30_Temp_C\n",
      "Day_avg_RH\n",
      "Day_max_RH\n",
      "Day_min_RH\n",
      "Day_more_95_RH\n",
      "Day_70_85_RH\n",
      "Day_70_95_RH\n",
      "Day_sum_Rain\n",
      "Day_more_0_Rain\n",
      "Night_avg_Temp_C\n",
      "Night_max_Temp_C\n",
      "Night_min_Temp_C\n",
      "Night_less_5_Temp_C\n",
      "Night_less_13_Temp_C\n",
      "Night_more_30_Temp_C\n",
      "Night_more_35_Temp_C\n",
      "Night_between_15_25_Temp_C\n",
      "Night_between_18_25_Temp_C\n",
      "Night_between_18_30_Temp_C\n",
      "Night_avg_RH\n",
      "Night_max_RH\n",
      "Night_min_RH\n",
      "Night_more_95_RH\n",
      "Night_between_70_85_RH\n",
      "Night_between_70_95_RH\n",
      "Night_sum_Rain\n",
      "Night_more_0_Rain\n",
      "Quot_avg_Temp_C\n",
      "Quot_max_Temp_C\n",
      "Quot_min_Temp_C\n",
      "Quot_less_5_Temp_C\n",
      "Quot_less_13_Temp_C\n",
      "Quot_more_30_Temp_C\n",
      "Quot_more_35_Temp_C\n",
      "Quot_between_15_25_Temp_C\n",
      "Quot_between_18_25_Temp_C\n",
      "Quot_between_18_30_Temp_C\n",
      "Quot_avg_RH\n",
      "Quot_max_RH\n",
      "Quot_min_RH\n",
      "Quot_more_95_RH\n",
      "Quot_between_70_85_RH\n",
      "Quot_between_70_95_RH\n",
      "Quot_avg_DewPoint_C\n",
      "Quot_avg_SolarRadiation_W_m2\n",
      "Quot_sum_Rain\n",
      "Quot_more_0_Rain\n",
      "Quot_sum_FeuillageMouille\n",
      "Quot_avg_WindSpeed_m_s\n",
      "Quot_avg_GustSpeed_m_s\n",
      "Quot_max_GustSpeed_m_s\n",
      "Quot_avg_WindDirection\n",
      "DegresJoursQuot\n",
      "DegresJourCumSum\n",
      "Rolling_Quot_temp_mean_3D\n",
      "Rolling_Quot_Temp_max_3D\n",
      "Rolling_Quot_Temp_min_3D\n",
      "Rolling_Quot_less_5_Temp_C_3D\n",
      "Rolling_Quot_less_13_Temp_C_3D\n",
      "Rolling_Quot_more_30_Temp_C_3D\n",
      "Rolling_Quot_more_35_Temp_C_3D\n",
      "Rolling_Quot_between_15_25_Temp_C_3D\n",
      "Rolling_Quot_between_18_25_Temp_C_3D\n",
      "Rolling_Quot_between_18_30_Temp_C_3D\n",
      "Rolling_Quot_RH_mean_3D\n",
      "Rolling_Quot_RH_min_3D\n",
      "Rolling_Quot_RH_max_3D\n",
      "Rolling_Quot_DewPoint_sum_3D\n",
      "Rolling_Quot_SolarRadiation_sum_3D\n",
      "Rolling_Quot_SolarRadiation_mean_3D\n",
      "Rolling_Quot_Rain_sum_3D\n",
      "Rolling_Quot_Rain_mean_3D\n",
      "Rolling_Quot_more_0_Rain_3D\n",
      "Rolling_Quot_WindSpeed_mean_3D\n",
      "Rolling_Quot_GustSpeed_mean_3D\n",
      "Rolling_DegresJours_sum_3D\n",
      "Rolling_Quot_temp_mean_6D\n",
      "Rolling_Quot_Temp_max_6D\n",
      "Rolling_Quot_Temp_min_6D\n",
      "Rolling_Quot_less_5_Temp_C_6D\n",
      "Rolling_Quot_less_13_Temp_C_6D\n",
      "Rolling_Quot_more_30_Temp_C_6D\n",
      "Rolling_Quot_more_35_Temp_C_6D\n",
      "Rolling_Quot_between_15_25_Temp_C_6D\n",
      "Rolling_Quot_between_18_25_Temp_C_6D\n",
      "Rolling_Quot_between_18_30_Temp_C_6D\n",
      "Rolling_Quot_RH_mean_6D\n",
      "Rolling_Quot_RH_min_6D\n",
      "Rolling_Quot_RH_max_6D\n",
      "Rolling_Quot_DewPoint_sum_6D\n",
      "Rolling_Quot_SolarRadiation_sum_6D\n",
      "Rolling_Quot_SolarRadiation_mean_6D\n",
      "Rolling_Quot_Rain_sum_6D\n",
      "Rolling_Quot_Rain_mean_6D\n",
      "Rolling_Quot_more_0_Rain_6D\n",
      "Rolling_Quot_WindSpeed_mean_6D\n",
      "Rolling_Quot_GustSpeed_mean_6D\n",
      "Rolling_DegresJours_sum_6D\n",
      "Rolling_Quot_temp_mean_14D\n",
      "Rolling_Quot_Temp_max_14D\n",
      "Rolling_Quot_Temp_min_14D\n",
      "Rolling_Quot_less_5_Temp_C_14D\n",
      "Rolling_Quot_less_13_Temp_C_14D\n",
      "Rolling_Quot_more_30_Temp_C_14D\n",
      "Rolling_Quot_more_35_Temp_C_14D\n",
      "Rolling_Quot_between_15_25_Temp_C_14D\n",
      "Rolling_Quot_between_18_25_Temp_C_14D\n",
      "Rolling_Quot_between_18_30_Temp_C_14D\n",
      "Rolling_Quot_RH_mean_14D\n",
      "Rolling_Quot_RH_min_14D\n",
      "Rolling_Quot_RH_max_14D\n",
      "Rolling_Quot_DewPoint_sum_14D\n",
      "Rolling_Quot_SolarRadiation_sum_14D\n",
      "Rolling_Quot_SolarRadiation_mean_14D\n",
      "Rolling_Quot_Rain_sum_14D\n",
      "Rolling_Quot_Rain_mean_14D\n",
      "Rolling_Quot_more_0_Rain_14D\n",
      "Rolling_Quot_WindSpeed_mean_14D\n",
      "Rolling_Quot_GustSpeed_mean_14D\n",
      "Rolling_DegresJours_sum_14D\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "for i in meteo_df.columns.tolist():\n",
    "  print(i)\n",
    "\n",
    "print (len(meteo_df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrot_df.rename(columns={'SampleDate':'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrot_df = carrot_df.drop(carrot_df[carrot_df['FarmID'] == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    276\n",
       "1    218\n",
       "2      4\n",
       "3      1\n",
       "Name: cote_c_carotae, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carrot_df['cote_c_carotae'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    276\n",
       "1    223\n",
       "Name: cote_c_carotae, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carrot_df.loc[carrot_df['cote_c_carotae'] >= 1, 'cote_c_carotae'] = 1\n",
    "carrot_df['cote_c_carotae'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9040, 1.1188], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Assuming `true_labels` contains your actual class labels\n",
    "unique_classes = np.unique(carrot_df['cote_c_carotae'])\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=unique_classes, y=carrot_df['cote_c_carotae'])\n",
    "\n",
    "# Convert class weights to a tensor and move it to the same device as your model\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).double().to(device)\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202 204 207 209 214 216 222 225 237 243 244 251 253 259 260 265 266 271\n",
      " 272]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FarmID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day_avg_Temp_C</th>\n",
       "      <th>Day_max_Temp_C</th>\n",
       "      <th>Day_min_Temp_C</th>\n",
       "      <th>Day_less_5_Temp_C</th>\n",
       "      <th>Day_less_13_Temp_C</th>\n",
       "      <th>Day_more_30_Temp_C</th>\n",
       "      <th>Day_more_35_Temp_C</th>\n",
       "      <th>Day_15_25_Temp_C</th>\n",
       "      <th>...</th>\n",
       "      <th>Rolling_Quot_RH_max_14D</th>\n",
       "      <th>Rolling_Quot_DewPoint_sum_14D</th>\n",
       "      <th>Rolling_Quot_SolarRadiation_sum_14D</th>\n",
       "      <th>Rolling_Quot_SolarRadiation_mean_14D</th>\n",
       "      <th>Rolling_Quot_Rain_sum_14D</th>\n",
       "      <th>Rolling_Quot_Rain_mean_14D</th>\n",
       "      <th>Rolling_Quot_more_0_Rain_14D</th>\n",
       "      <th>Rolling_Quot_WindSpeed_mean_14D</th>\n",
       "      <th>Rolling_Quot_GustSpeed_mean_14D</th>\n",
       "      <th>Rolling_DegresJours_sum_14D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>19.633077</td>\n",
       "      <td>23.93</td>\n",
       "      <td>15.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>99.8</td>\n",
       "      <td>241.599583</td>\n",
       "      <td>2473.500000</td>\n",
       "      <td>176.678571</td>\n",
       "      <td>47.2</td>\n",
       "      <td>3.371429</td>\n",
       "      <td>2.071429</td>\n",
       "      <td>0.449405</td>\n",
       "      <td>2.423810</td>\n",
       "      <td>232.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>204</td>\n",
       "      <td>22.406923</td>\n",
       "      <td>26.11</td>\n",
       "      <td>16.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>99.8</td>\n",
       "      <td>239.610833</td>\n",
       "      <td>2842.041667</td>\n",
       "      <td>203.002976</td>\n",
       "      <td>28.2</td>\n",
       "      <td>2.014286</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>2.609821</td>\n",
       "      <td>239.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>24.604615</td>\n",
       "      <td>28.10</td>\n",
       "      <td>16.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>99.8</td>\n",
       "      <td>243.713333</td>\n",
       "      <td>2762.166667</td>\n",
       "      <td>197.297619</td>\n",
       "      <td>39.6</td>\n",
       "      <td>2.828571</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>0.703869</td>\n",
       "      <td>3.064286</td>\n",
       "      <td>244.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>21.417692</td>\n",
       "      <td>25.26</td>\n",
       "      <td>15.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>99.8</td>\n",
       "      <td>234.027917</td>\n",
       "      <td>2749.791667</td>\n",
       "      <td>196.413690</td>\n",
       "      <td>39.4</td>\n",
       "      <td>2.814286</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>0.574405</td>\n",
       "      <td>2.685714</td>\n",
       "      <td>235.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>20.373077</td>\n",
       "      <td>23.62</td>\n",
       "      <td>14.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>99.8</td>\n",
       "      <td>206.191667</td>\n",
       "      <td>2762.833333</td>\n",
       "      <td>197.345238</td>\n",
       "      <td>45.2</td>\n",
       "      <td>3.228571</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>3.085119</td>\n",
       "      <td>212.380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FarmID  Date  Day_avg_Temp_C  Day_max_Temp_C  Day_min_Temp_C  \\\n",
       "37       0   202       19.633077           23.93           15.27   \n",
       "39       0   204       22.406923           26.11           16.18   \n",
       "42       0   207       24.604615           28.10           16.75   \n",
       "44       0   209       21.417692           25.26           15.08   \n",
       "49       0   214       20.373077           23.62           14.96   \n",
       "\n",
       "    Day_less_5_Temp_C  Day_less_13_Temp_C  Day_more_30_Temp_C  \\\n",
       "37                  0                   0                   0   \n",
       "39                  0                   0                   0   \n",
       "42                  0                   0                   0   \n",
       "44                  0                   0                   0   \n",
       "49                  0                   0                   0   \n",
       "\n",
       "    Day_more_35_Temp_C  Day_15_25_Temp_C  ...  Rolling_Quot_RH_max_14D  \\\n",
       "37                   0                13  ...                     99.8   \n",
       "39                   0                 9  ...                     99.8   \n",
       "42                   0                 6  ...                     99.8   \n",
       "44                   0                12  ...                     99.8   \n",
       "49                   0                12  ...                     99.8   \n",
       "\n",
       "    Rolling_Quot_DewPoint_sum_14D  Rolling_Quot_SolarRadiation_sum_14D  \\\n",
       "37                     241.599583                          2473.500000   \n",
       "39                     239.610833                          2842.041667   \n",
       "42                     243.713333                          2762.166667   \n",
       "44                     234.027917                          2749.791667   \n",
       "49                     206.191667                          2762.833333   \n",
       "\n",
       "    Rolling_Quot_SolarRadiation_mean_14D  Rolling_Quot_Rain_sum_14D  \\\n",
       "37                            176.678571                       47.2   \n",
       "39                            203.002976                       28.2   \n",
       "42                            197.297619                       39.6   \n",
       "44                            196.413690                       39.4   \n",
       "49                            197.345238                       45.2   \n",
       "\n",
       "    Rolling_Quot_Rain_mean_14D  Rolling_Quot_more_0_Rain_14D  \\\n",
       "37                    3.371429                      2.071429   \n",
       "39                    2.014286                      1.285714   \n",
       "42                    2.828571                      1.642857   \n",
       "44                    2.814286                      1.642857   \n",
       "49                    3.228571                      1.857143   \n",
       "\n",
       "    Rolling_Quot_WindSpeed_mean_14D  Rolling_Quot_GustSpeed_mean_14D  \\\n",
       "37                         0.449405                         2.423810   \n",
       "39                         0.552083                         2.609821   \n",
       "42                         0.703869                         3.064286   \n",
       "44                         0.574405                         2.685714   \n",
       "49                         0.741071                         3.085119   \n",
       "\n",
       "    Rolling_DegresJours_sum_14D  \n",
       "37                      232.500  \n",
       "39                      239.375  \n",
       "42                      244.495  \n",
       "44                      235.605  \n",
       "49                      212.380  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we remove every date in the meteo dataframe where no sample have been taken\n",
    "unique_sample_date = carrot_df['Date'].unique()\n",
    "print(unique_sample_date)\n",
    "unique_sample_date = meteo_df[meteo_df['Date'].isin(unique_sample_date)]\n",
    "\n",
    "unique_sample_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FarmID</th>\n",
       "      <th>GreenLeavesNum_carrots</th>\n",
       "      <th>Plant_ID</th>\n",
       "      <th>cote_c_carotae</th>\n",
       "      <th>Date</th>\n",
       "      <th>carrot_stage</th>\n",
       "      <th>Day_avg_Temp_C</th>\n",
       "      <th>Day_max_Temp_C</th>\n",
       "      <th>Day_min_Temp_C</th>\n",
       "      <th>Day_less_5_Temp_C</th>\n",
       "      <th>...</th>\n",
       "      <th>Rolling_Quot_RH_max_14D</th>\n",
       "      <th>Rolling_Quot_DewPoint_sum_14D</th>\n",
       "      <th>Rolling_Quot_SolarRadiation_sum_14D</th>\n",
       "      <th>Rolling_Quot_SolarRadiation_mean_14D</th>\n",
       "      <th>Rolling_Quot_Rain_sum_14D</th>\n",
       "      <th>Rolling_Quot_Rain_mean_14D</th>\n",
       "      <th>Rolling_Quot_more_0_Rain_14D</th>\n",
       "      <th>Rolling_Quot_WindSpeed_mean_14D</th>\n",
       "      <th>Rolling_Quot_GustSpeed_mean_14D</th>\n",
       "      <th>Rolling_DegresJours_sum_14D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>18.976154</td>\n",
       "      <td>23.16</td>\n",
       "      <td>15.84</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.9</td>\n",
       "      <td>244.769167</td>\n",
       "      <td>2526.625</td>\n",
       "      <td>180.473214</td>\n",
       "      <td>59.6</td>\n",
       "      <td>4.257143</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>0.747024</td>\n",
       "      <td>3.088095</td>\n",
       "      <td>229.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>18.976154</td>\n",
       "      <td>23.16</td>\n",
       "      <td>15.84</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.9</td>\n",
       "      <td>244.769167</td>\n",
       "      <td>2526.625</td>\n",
       "      <td>180.473214</td>\n",
       "      <td>59.6</td>\n",
       "      <td>4.257143</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>0.747024</td>\n",
       "      <td>3.088095</td>\n",
       "      <td>229.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>18.976154</td>\n",
       "      <td>23.16</td>\n",
       "      <td>15.84</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.9</td>\n",
       "      <td>244.769167</td>\n",
       "      <td>2526.625</td>\n",
       "      <td>180.473214</td>\n",
       "      <td>59.6</td>\n",
       "      <td>4.257143</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>0.747024</td>\n",
       "      <td>3.088095</td>\n",
       "      <td>229.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>18.976154</td>\n",
       "      <td>23.16</td>\n",
       "      <td>15.84</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.9</td>\n",
       "      <td>244.769167</td>\n",
       "      <td>2526.625</td>\n",
       "      <td>180.473214</td>\n",
       "      <td>59.6</td>\n",
       "      <td>4.257143</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>0.747024</td>\n",
       "      <td>3.088095</td>\n",
       "      <td>229.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>18.976154</td>\n",
       "      <td>23.16</td>\n",
       "      <td>15.84</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.9</td>\n",
       "      <td>244.769167</td>\n",
       "      <td>2526.625</td>\n",
       "      <td>180.473214</td>\n",
       "      <td>59.6</td>\n",
       "      <td>4.257143</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>0.747024</td>\n",
       "      <td>3.088095</td>\n",
       "      <td>229.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>22.380000</td>\n",
       "      <td>25.36</td>\n",
       "      <td>15.34</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.1</td>\n",
       "      <td>200.061250</td>\n",
       "      <td>2111.625</td>\n",
       "      <td>150.830357</td>\n",
       "      <td>26.2</td>\n",
       "      <td>1.871429</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.962798</td>\n",
       "      <td>3.867560</td>\n",
       "      <td>186.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>22.380000</td>\n",
       "      <td>25.36</td>\n",
       "      <td>15.34</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.1</td>\n",
       "      <td>200.061250</td>\n",
       "      <td>2111.625</td>\n",
       "      <td>150.830357</td>\n",
       "      <td>26.2</td>\n",
       "      <td>1.871429</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.962798</td>\n",
       "      <td>3.867560</td>\n",
       "      <td>186.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>22.380000</td>\n",
       "      <td>25.36</td>\n",
       "      <td>15.34</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.1</td>\n",
       "      <td>200.061250</td>\n",
       "      <td>2111.625</td>\n",
       "      <td>150.830357</td>\n",
       "      <td>26.2</td>\n",
       "      <td>1.871429</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.962798</td>\n",
       "      <td>3.867560</td>\n",
       "      <td>186.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>22.380000</td>\n",
       "      <td>25.36</td>\n",
       "      <td>15.34</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.1</td>\n",
       "      <td>200.061250</td>\n",
       "      <td>2111.625</td>\n",
       "      <td>150.830357</td>\n",
       "      <td>26.2</td>\n",
       "      <td>1.871429</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.962798</td>\n",
       "      <td>3.867560</td>\n",
       "      <td>186.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>22.380000</td>\n",
       "      <td>25.36</td>\n",
       "      <td>15.34</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.1</td>\n",
       "      <td>200.061250</td>\n",
       "      <td>2111.625</td>\n",
       "      <td>150.830357</td>\n",
       "      <td>26.2</td>\n",
       "      <td>1.871429</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.962798</td>\n",
       "      <td>3.867560</td>\n",
       "      <td>186.985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FarmID  GreenLeavesNum_carrots  Plant_ID  cote_c_carotae  Date  \\\n",
       "0         2                       4         1               0   202   \n",
       "1         2                       4         2               0   202   \n",
       "2         2                       4         3               0   202   \n",
       "3         2                       4         4               0   202   \n",
       "4         2                       4         5               0   202   \n",
       "..      ...                     ...       ...             ...   ...   \n",
       "394       2                       6        21               1   260   \n",
       "395       2                       6        22               1   260   \n",
       "396       2                       6        23               1   260   \n",
       "397       2                       6        24               1   260   \n",
       "398       2                       6        25               0   260   \n",
       "\n",
       "     carrot_stage  Day_avg_Temp_C  Day_max_Temp_C  Day_min_Temp_C  \\\n",
       "0               0       18.976154           23.16           15.84   \n",
       "1               0       18.976154           23.16           15.84   \n",
       "2               0       18.976154           23.16           15.84   \n",
       "3               0       18.976154           23.16           15.84   \n",
       "4               0       18.976154           23.16           15.84   \n",
       "..            ...             ...             ...             ...   \n",
       "394             2       22.380000           25.36           15.34   \n",
       "395             2       22.380000           25.36           15.34   \n",
       "396             2       22.380000           25.36           15.34   \n",
       "397             2       22.380000           25.36           15.34   \n",
       "398             2       22.380000           25.36           15.34   \n",
       "\n",
       "     Day_less_5_Temp_C  ...  Rolling_Quot_RH_max_14D  \\\n",
       "0                    0  ...                     99.9   \n",
       "1                    0  ...                     99.9   \n",
       "2                    0  ...                     99.9   \n",
       "3                    0  ...                     99.9   \n",
       "4                    0  ...                     99.9   \n",
       "..                 ...  ...                      ...   \n",
       "394                  0  ...                     99.1   \n",
       "395                  0  ...                     99.1   \n",
       "396                  0  ...                     99.1   \n",
       "397                  0  ...                     99.1   \n",
       "398                  0  ...                     99.1   \n",
       "\n",
       "     Rolling_Quot_DewPoint_sum_14D  Rolling_Quot_SolarRadiation_sum_14D  \\\n",
       "0                       244.769167                             2526.625   \n",
       "1                       244.769167                             2526.625   \n",
       "2                       244.769167                             2526.625   \n",
       "3                       244.769167                             2526.625   \n",
       "4                       244.769167                             2526.625   \n",
       "..                             ...                                  ...   \n",
       "394                     200.061250                             2111.625   \n",
       "395                     200.061250                             2111.625   \n",
       "396                     200.061250                             2111.625   \n",
       "397                     200.061250                             2111.625   \n",
       "398                     200.061250                             2111.625   \n",
       "\n",
       "     Rolling_Quot_SolarRadiation_mean_14D  Rolling_Quot_Rain_sum_14D  \\\n",
       "0                              180.473214                       59.6   \n",
       "1                              180.473214                       59.6   \n",
       "2                              180.473214                       59.6   \n",
       "3                              180.473214                       59.6   \n",
       "4                              180.473214                       59.6   \n",
       "..                                    ...                        ...   \n",
       "394                            150.830357                       26.2   \n",
       "395                            150.830357                       26.2   \n",
       "396                            150.830357                       26.2   \n",
       "397                            150.830357                       26.2   \n",
       "398                            150.830357                       26.2   \n",
       "\n",
       "     Rolling_Quot_Rain_mean_14D  Rolling_Quot_more_0_Rain_14D  \\\n",
       "0                      4.257143                      2.357143   \n",
       "1                      4.257143                      2.357143   \n",
       "2                      4.257143                      2.357143   \n",
       "3                      4.257143                      2.357143   \n",
       "4                      4.257143                      2.357143   \n",
       "..                          ...                           ...   \n",
       "394                    1.871429                      1.500000   \n",
       "395                    1.871429                      1.500000   \n",
       "396                    1.871429                      1.500000   \n",
       "397                    1.871429                      1.500000   \n",
       "398                    1.871429                      1.500000   \n",
       "\n",
       "     Rolling_Quot_WindSpeed_mean_14D  Rolling_Quot_GustSpeed_mean_14D  \\\n",
       "0                           0.747024                         3.088095   \n",
       "1                           0.747024                         3.088095   \n",
       "2                           0.747024                         3.088095   \n",
       "3                           0.747024                         3.088095   \n",
       "4                           0.747024                         3.088095   \n",
       "..                               ...                              ...   \n",
       "394                         0.962798                         3.867560   \n",
       "395                         0.962798                         3.867560   \n",
       "396                         0.962798                         3.867560   \n",
       "397                         0.962798                         3.867560   \n",
       "398                         0.962798                         3.867560   \n",
       "\n",
       "     Rolling_DegresJours_sum_14D  \n",
       "0                        229.010  \n",
       "1                        229.010  \n",
       "2                        229.010  \n",
       "3                        229.010  \n",
       "4                        229.010  \n",
       "..                           ...  \n",
       "394                      186.985  \n",
       "395                      186.985  \n",
       "396                      186.985  \n",
       "397                      186.985  \n",
       "398                      186.985  \n",
       "\n",
       "[399 rows x 135 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = carrot_df.merge(meteo_df, on=['FarmID', 'Date'])\n",
    "\n",
    "df_squamosa = combined_df.get('cote_c_carotae')\n",
    "\n",
    "combined_df = combined_df.drop(['incidence_a_dauci', 'incidence_s_sclerotiorum'], axis=1)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "Botcast\n",
      "Clarisse_dataset_1\n",
      "Clarisse_dataset_2\n",
      "Clarisse_dataset_3\n",
      "Clarisse_dataset_4\n",
      "f_classif_10\n",
      "f_classif_20\n",
      "f_classif_5\n",
      "lasso_alpha0.005\n",
      "lasso_alpha0.01\n",
      "lasso_alpha0.02\n",
      "MultiSURF10\n",
      "MultiSURF20\n",
      "MultiSURF5\n",
      "mutual_info_class_10\n",
      "mutual_info_class_20\n",
      "mutual_info_class_5\n",
      "ReliefF10\n",
      "ReliefF20\n",
      "ReliefF5\n",
      "Steentjes\n",
      "22 datasets appended\n"
     ]
    }
   ],
   "source": [
    "# explore the directory containing the datasets vars and populate a result dataframe\n",
    "\n",
    "# assign directory\n",
    "directory = '../datasets_vars/'\n",
    "datasets = []\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(file_path):\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        print(file_name)\n",
    "        datasets.append(file_name)\n",
    "\n",
    "print(f'{len(datasets)} datasets appended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sub_df(combined_df, vars):\n",
    "    return combined_df.loc[:, vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sub_df, exception_list):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    columns_to_normalize = [col for col in sub_df.columns if col not in exception_list]\n",
    "\n",
    "    # Normalize the selected columns\n",
    "    sub_df[columns_to_normalize] = min_max_scaler.fit_transform(sub_df[columns_to_normalize])\n",
    "    return (sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plant_distances(fieldID, threshold, distance_txt_file, valid_ids):\n",
    "  array1 = []\n",
    "  array2 = []\n",
    "  for line in distance_txt_file:\n",
    "      farmID, plant1, plant2, dist = line.split(',', 3)\n",
    "\n",
    "      # conditions: find the right field,\n",
    "      # plants are close enough together\n",
    "      # avoids error if there was a partial observation that day (less nodes than the dist matrix has)\n",
    "      # removes the last overflowing extra node if partial observation\n",
    "\n",
    "      if (int(farmID) == fieldID and\n",
    "      float(dist) <= threshold and\n",
    "      all(int(x) in valid_ids for x in [plant1, plant2]) and\n",
    "      ((max(int(plant1), int(plant2)) < max(valid_ids)))):\n",
    "          array1.append(int(plant1))\n",
    "          array2.append(int(plant2))\n",
    "\n",
    "  return array1, array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_MetaData(carrot_df):\n",
    "    # extract the farm IDs, dates, and plants information from the dataframes\n",
    "    farm_ids = carrot_df['FarmID'].unique()\n",
    "    dates = carrot_df['Date'].unique()\n",
    "    return(farm_ids, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ID_sparse(day_farm_data_subset, ID_column, donor, receiver):\n",
    "    max_index = len(day_farm_data_subset[ID_column].tolist())\n",
    "    node_id_list = day_farm_data_subset[ID_column].tolist()\n",
    "    gaps = []\n",
    "    for i in range(max_index):\n",
    "        if i not in node_id_list:\n",
    "            gaps.append(i)\n",
    "    \n",
    "    if len(gaps) > 0:\n",
    "        id_to_replace = [i for i in node_id_list if i >= max_index]\n",
    "        #print(f'id_to_replace: {id_to_replace}')\n",
    "        \n",
    "        #gaps and id_to_replace will always be the same size logically\n",
    "        for idx in range(len(gaps)):\n",
    "            old_id = id_to_replace[idx]\n",
    "            new_id = gaps[idx]\n",
    "            #day_farm_data_subset['Plant_ID'] = day_farm_data_subset['Plant_ID'].replace(old_id, new_id)\n",
    "            day_farm_data_subset.loc[day_farm_data_subset[ID_column] == old_id, ID_column] = new_id\n",
    "\n",
    "            donor = [new_id if x == old_id else x for x in donor]\n",
    "            receiver = [new_id if x == old_id else x for x in receiver]\n",
    "\n",
    "    return day_farm_data_subset, donor, receiver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche par seuil coulissant\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(donor, receiver, day_farm_data_subset, squamosa_rate_label, train_masks, valid_masks, test_masks, plant_tags):\n",
    "    donor_np = np.array(donor)\n",
    "    receiver_np = np.array(receiver)\n",
    "\n",
    "    # we need edges going both ways \n",
    "    from_plant1 = torch.tensor(donor_np, dtype = int)\n",
    "    to_plant2 = torch.tensor(receiver_np, dtype = int)\n",
    "\n",
    "    plant_edges = torch.concat((from_plant1, to_plant2)).reshape(-1, len(from_plant1)).long()\n",
    "\n",
    "    temp = day_farm_data_subset['Plant_ID'].to_list()\n",
    "    #remove vars we dont want in the graph (the metadata)\n",
    "    day_farm_data_clean = day_farm_data_subset.drop(['FarmID', 'Plant_ID', 'Date', 'cote_c_carotae'], axis=1)\n",
    "\n",
    "    #convert class values to tensor\n",
    "    squamosa_rate_label = torch.tensor(squamosa_rate_label, dtype = int)\n",
    "\n",
    "    # create plant node as tensor\n",
    "    plants_tensor = torch.tensor(np.array(day_farm_data_clean), dtype = float)\n",
    "\n",
    "    graph = Data(x=plants_tensor, edge_index=plant_edges, y=squamosa_rate_label)\n",
    "\n",
    "    transform = T.Compose([T.ToUndirected(), T.AddSelfLoops()])\n",
    "    graph = transform(graph)\n",
    "\n",
    "    graph.train_mask = torch.Tensor(train_masks)\n",
    "    graph.val_mask = torch.Tensor(valid_masks)\n",
    "    graph.test_mask = torch.Tensor(test_masks)\n",
    "    graph.plant_tags = torch.Tensor(plant_tags)\n",
    "\n",
    "    return graph, len(plant_edges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_HGNN_aug_sliding_threshold(sub_df_norm, masking_data, distance_txt_file, minimum_distance, maximum_distance, step):\n",
    "\n",
    "    farm_ids, dates = extract_MetaData(sub_df_norm)\n",
    "\n",
    "    # list to store the graph for each farm and date\n",
    "    graphs = []\n",
    "    unique_farms_idx = {}\n",
    "    aug_proof_concept = {}\n",
    "    aug_proof_concept_iter = -1\n",
    "    nbr_total_nodes = 0\n",
    "    nbr_total_edges = 0\n",
    "\n",
    "\n",
    "    # loop through each farm and date\n",
    "    for (farm_id, date) in itertools.product(farm_ids, dates):\n",
    "\n",
    "        train_masks, valid_masks, test_masks, plant_tags = create_masks(masking_data, farm_id)\n",
    "        plant_ids = sub_df_norm[(sub_df_norm['FarmID'] == farm_id) & (sub_df_norm['Date'] == date)]['Plant_ID'].tolist()\n",
    "        train_masks_subgraph = [train_masks[i - 1] for i in plant_ids]\n",
    "        valid_masks_subgraph = [valid_masks[i - 1] for i in plant_ids]\n",
    "        test_masks_subgraph = [test_masks[i - 1] for i in plant_ids]\n",
    "        plant_tags_subgraph = [plant_tags[i - 1] for i in plant_ids]\n",
    "\n",
    "        day_farm_data = sub_df_norm[(sub_df_norm['FarmID'] == farm_id) & (sub_df_norm['Date'] == date)]\n",
    "        if day_farm_data.size > 0:\n",
    "            aug_proof_concept_iter +=1 # only for visualisation purposes\n",
    "\n",
    "            median_distance_threshold = int(round((minimum_distance+maximum_distance)/2))\n",
    "\n",
    "            donor, receiver = get_plant_distances(farm_id, median_distance_threshold, distance_txt_file, day_farm_data['Plant_ID'].tolist())\n",
    "\n",
    "            if len(plant_ids) != max(plant_ids):\n",
    "                day_farm_data, donor, receiver = clean_ID_sparse(day_farm_data, \"Plant_ID\", donor, receiver)\n",
    "\n",
    "            # first iteration is not an augmentation task, we preserve as much data as possible for training\n",
    "            squamosa_rate_array = day_farm_data['cote_c_carotae'].to_numpy()\n",
    "            squamosa_rate_label = np.rint(squamosa_rate_array) # round up cote squamosa to nearest integer\n",
    "            day_farm_data_subset = day_farm_data # for compatibility with the aug operations\n",
    "\n",
    "            graph, nbr_edges = create_graph(donor, receiver, day_farm_data_subset, squamosa_rate_label, train_masks_subgraph, valid_masks_subgraph, test_masks_subgraph, plant_tags_subgraph)\n",
    "                \n",
    "            median_graph_edges = nbr_edges\n",
    "            graphs.append(graph)\n",
    "\n",
    "            if farm_id not in unique_farms_idx:\n",
    "                unique_farms_idx[farm_id] = graph\n",
    "                \n",
    "            if aug_proof_concept_iter == 0:\n",
    "                aug_proof_concept[median_distance_threshold] = graph\n",
    "\n",
    "            \n",
    "            for dist_threshold in range(minimum_distance, median_distance_threshold, step):\n",
    "                donor, receiver = get_plant_distances(farm_id, dist_threshold, distance_txt_file, day_farm_data['Plant_ID'].tolist())\n",
    "\n",
    "                # first iteration is not an augmentation task, we preserve as much data as possible for training\n",
    "                squamosa_rate_array = day_farm_data['cote_c_carotae'].to_numpy()\n",
    "                squamosa_rate_label = np.rint(squamosa_rate_array) # round up cote squamosa to nearest integer\n",
    "                day_farm_data_subset = day_farm_data # for compatibility with the aug operations\n",
    "\n",
    "                if len(donor) > 0:\n",
    "                    graph, nbr_edges = create_graph(donor, receiver, day_farm_data_subset, squamosa_rate_label, train_masks_subgraph, valid_masks_subgraph, test_masks_subgraph, plant_tags_subgraph)\n",
    "                else:\n",
    "                    nbr_edges = 0\n",
    "                \n",
    "                if nbr_edges >= 0.5 * median_graph_edges:\n",
    "                    graphs.append(graph)\n",
    "\n",
    "                # used to save a graph of each farm for visualisation purposes only\n",
    "                if farm_id not in unique_farms_idx:\n",
    "                    unique_farms_idx[farm_id] = graph\n",
    "                \n",
    "                if aug_proof_concept_iter == 0:\n",
    "                    aug_proof_concept[dist_threshold] = graph\n",
    "            \n",
    "            for dist_threshold in range(median_distance_threshold + 1, maximum_distance+ 1, step):\n",
    "                donor, receiver = get_plant_distances(farm_id, dist_threshold, distance_txt_file, day_farm_data['Plant_ID'].tolist())\n",
    "\n",
    "                # first iteration is not an augmentation task, we preserve as much data as possible for training\n",
    "                squamosa_rate_array = day_farm_data['cote_c_carotae'].to_numpy()\n",
    "                squamosa_rate_label = np.rint(squamosa_rate_array) # round up cote squamosa to nearest integer\n",
    "                day_farm_data_subset = day_farm_data # for compatibility with the aug operations\n",
    "\n",
    "                graph, nbr_edges = create_graph(donor, receiver, day_farm_data_subset, squamosa_rate_label, train_masks_subgraph, valid_masks_subgraph, test_masks_subgraph, plant_tags_subgraph)\n",
    "                \n",
    "                if nbr_edges <= 1.5 * median_graph_edges:\n",
    "                    graphs.append(graph)\n",
    "\n",
    "                # used to save a graph of each farm for visualisation purposes only\n",
    "                if farm_id not in unique_farms_idx:\n",
    "                    unique_farms_idx[farm_id] = graph\n",
    "                \n",
    "                if aug_proof_concept_iter == 0:\n",
    "                    aug_proof_concept[dist_threshold] = graph\n",
    "\n",
    "    return (graphs, unique_farms_idx, aug_proof_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche Louvain\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_communities(graph, min_threshold=0.1):\n",
    "    total_nodes = len(graph.nodes)\n",
    "    # Apply Louvain community detection\n",
    "    partition = community_louvain.best_partition(graph)\n",
    "    \n",
    "    # Create a dictionary to store communities\n",
    "    communities = {}\n",
    "    for node, community_id in partition.items():\n",
    "        if community_id not in communities:\n",
    "            communities[community_id] = []\n",
    "        communities[community_id].append(node)\n",
    "    \n",
    "    # Merge small communities or randomly assign nodes to a neighboring community\n",
    "    for community_id, nodes in list(communities.items()):\n",
    "        if len(nodes) < min_threshold * total_nodes:\n",
    "            neighbors = set()\n",
    "            for node in nodes:\n",
    "                neighbors.update(set(graph.neighbors(node)))\n",
    "            \n",
    "            # Filter out nodes already in the community \n",
    "            neighbors = neighbors - set(nodes)\n",
    "            if len(neighbors) > 0:\n",
    "                neighbor_community_id = partition[random.choice(list(neighbors))]\n",
    "                communities[neighbor_community_id].extend(nodes)\n",
    "                del communities[community_id]\n",
    "            else:\n",
    "                #random_community_id = random.choice(list(communities.keys()))\n",
    "                random_community_id = random.choice([cid for cid in communities.keys() if cid != community_id])\n",
    "                communities[random_community_id].extend(nodes)\n",
    "                del communities[community_id]\n",
    "    \n",
    "    return list(communities.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph_into_communities(graph, communities):\n",
    "    community_subgraphs = {}\n",
    "    \n",
    "    for community_id, nodes in enumerate(communities):\n",
    "        subgraph = graph.subgraph(nodes)\n",
    "        community_subgraphs[community_id] = subgraph\n",
    "    \n",
    "    return community_subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_initial_graph_with_partitions(graph, communities):\n",
    "    pos = nx.spring_layout(graph)  # You can use a different layout if desired\n",
    "    \n",
    "    # Draw nodes and edges of the entire graph\n",
    "    nx.draw(graph, pos, with_labels=True, node_color='lightgray', node_size=300)\n",
    "    \n",
    "    # Draw nodes of each community with different colors\n",
    "    for i, nodes in enumerate(communities):\n",
    "        nx.draw_networkx_nodes(graph, pos, nodelist=nodes, node_color=f'C{i}', node_size=300)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_community_subgraphs(community_subgraphs):\n",
    "    num_communities = len(community_subgraphs)\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_communities, figsize=(15, 5))\n",
    "\n",
    "    for i, (community_id, subgraph) in enumerate(community_subgraphs.items()):\n",
    "        axs[i].set_title(f'Community {community_id}')\n",
    "        pos = nx.spring_layout(subgraph)  # You can use a different layout if desired\n",
    "\n",
    "        color_dict = {\n",
    "            0:'blue',\n",
    "            1: 'orange',\n",
    "            2: 'green',\n",
    "            3: 'red',\n",
    "            4: 'purple',\n",
    "            5: 'brown',\n",
    "            6: 'pink',\n",
    "            7: 'gray',\n",
    "            8: 'olive',\n",
    "            9: 'cyan'\n",
    "        }\n",
    "        color_map = []\n",
    "        for node in subgraph:\n",
    "            color_map.append(color_dict[community_id])\n",
    "\n",
    "        nx.draw(subgraph, pos, node_color=color_map, with_labels=True, ax=axs[i])\n",
    "        #axs[i].axis('off')\n",
    "        \n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_HGNN_louvain(sub_df_norm, masking_data, distance_txt_file, dist_threshold_choice = [20, 25, 30, 35, 40]):\n",
    "\n",
    "    farm_ids, dates = extract_MetaData(sub_df_norm)\n",
    "\n",
    "    # list to store the graph for each farm and date\n",
    "    graphs = []\n",
    "    unique_farms_idx = {}\n",
    "    aug_proof_concept = {}\n",
    "    aug_proof_concept_iter = -1\n",
    "    nbr_total_nodes = 0\n",
    "    nbr_total_edges = 0\n",
    "    size_communities = []\n",
    "    nbr_subgraphs = 0\n",
    "\n",
    "\n",
    "    # loop through each farm and date\n",
    "    for (farm_id, date) in itertools.product(farm_ids, dates):\n",
    "\n",
    "        train_masks, valid_masks, test_masks, plant_tags = create_masks(masking_data, farm_id)\n",
    "        day_farm_data = sub_df_norm[(sub_df_norm['FarmID'] == farm_id) & (sub_df_norm['Date'] == date)]\n",
    "        if day_farm_data.size > 0:\n",
    "            day_farm_data_subset = day_farm_data.copy(deep=True)\n",
    "            aug_proof_concept_iter +=1 # only for visualisation purposes\n",
    "\n",
    "            #get edges\n",
    "            dist_threshold = random.choice(dist_threshold_choice)\n",
    "            donor, receiver = get_plant_distances(farm_id, dist_threshold, distance_txt_file, day_farm_data['Plant_ID'].tolist())\n",
    "            plant_edges = list(zip(donor, receiver))\n",
    "\n",
    "            # columns that should not be features in the graph\n",
    "            exclude_columns = ['FarmID', 'Plant_ID', 'Date', 'cote_c_carotae']\n",
    "            node_id_column = day_farm_data_subset['Plant_ID']\n",
    "            node_features = day_farm_data_subset.drop(exclude_columns, axis=1).to_dict('records')\n",
    "            \n",
    "\n",
    "            graph_nx = nx.Graph()\n",
    "            for node_id, features in zip(node_id_column, node_features):\n",
    "                graph_nx.add_node(node_id, **features)\n",
    "            \n",
    "            for edge in plant_edges:\n",
    "                graph_nx.add_edge(*edge)\n",
    "            \n",
    "\n",
    "            communities = find_communities(graph_nx)\n",
    "\n",
    "            community_subgraphs = split_graph_into_communities(graph_nx, communities)\n",
    "\n",
    "            #plot_initial_graph_with_partitions(graph_nx, communities)\n",
    "            #plot_community_subgraphs(community_subgraphs)\n",
    "            \n",
    "            for com_id, subgraph in community_subgraphs.items():\n",
    "                node_id_list = subgraph.nodes\n",
    "                edges_list = subgraph.edges\n",
    "\n",
    "                subgraph_df = day_farm_data.loc[day_farm_data['Plant_ID'].isin(node_id_list)]\n",
    "\n",
    "                # extract appropriate masks\n",
    "                train_masks_subgraph = [train_masks[i - 1] for i in node_id_list]\n",
    "                valid_masks_subgraph = [valid_masks[i - 1] for i in node_id_list]\n",
    "                test_masks_subgraph = [test_masks[i - 1] for i in node_id_list]\n",
    "                plant_tags_subgraph = [plant_tags[i - 1] for i in node_id_list]\n",
    "                \n",
    "                # resetting node IDS\n",
    "                new_plant_id = list(range(0, len(node_id_list)))\n",
    "                new_plant_id_tuples = list(zip(node_id_list, new_plant_id))\n",
    "\n",
    "                for old_id, new_id in new_plant_id_tuples:\n",
    "                    subgraph_df.loc[subgraph_df[\"Plant_ID\"] == old_id, \"Plant_ID\"] = new_id\n",
    "\n",
    "                id_dict = dict(new_plant_id_tuples)\n",
    "                updated_edges_list = [(id_dict.get(old_id_1, old_id_1), id_dict.get(old_id_2, old_id_2)) for old_id_1, old_id_2 in edges_list]\n",
    "\n",
    "                # shared operations between augmented and non-augmented graphs\n",
    "                if len(updated_edges_list) == 0:\n",
    "                    continue # we skip graphs with no edges at all\n",
    "\n",
    "                donor, receiver = zip(*updated_edges_list)\n",
    "                donor_np = np.array(donor)\n",
    "                receiver_np = np.array(receiver)\n",
    "\n",
    "                # we need edges going both ways \n",
    "                from_plant1 = torch.tensor(donor_np, dtype = int)\n",
    "                to_plant2 = torch.tensor(receiver_np, dtype = int)\n",
    "\n",
    "                plant_edges = torch.concat((from_plant1, to_plant2)).reshape(-1, len(from_plant1)).long()\n",
    "\n",
    "                #convert class values to tensor\n",
    "                squamosa_rate_array = subgraph_df['cote_c_carotae'].to_numpy()\n",
    "                squamosa_rate_label = np.rint(squamosa_rate_array) # round up cote squamosa to nearest integer\n",
    "                squamosa_rate_label = torch.tensor(squamosa_rate_label, dtype = int)\n",
    "\n",
    "                #remove vars we dont want in the graph (the metadata)\n",
    "                day_farm_data_clean = subgraph_df.drop(['FarmID', 'Plant_ID', 'Date', 'cote_c_carotae'], axis=1)\n",
    "\n",
    "                # create plant node as tensor\n",
    "                plants_tensor = torch.tensor(np.array(day_farm_data_clean), dtype = float)\n",
    "\n",
    "                graph = Data(x=plants_tensor, edge_index=plant_edges, y=squamosa_rate_label)\n",
    "\n",
    "                transform = T.Compose([T.ToUndirected(), T.AddSelfLoops()])\n",
    "                graph = transform(graph)\n",
    "\n",
    "                graph.train_mask = torch.Tensor(train_masks_subgraph)\n",
    "                graph.val_mask = torch.Tensor(valid_masks_subgraph)\n",
    "                graph.test_mask = torch.Tensor(test_masks_subgraph)\n",
    "                graph.plant_tags = torch.Tensor(plant_tags_subgraph)\n",
    "\n",
    "                if (len(train_masks_subgraph)) != len(squamosa_rate_array):\n",
    "                    print('error: training mask not right size')\n",
    "\n",
    "                graphs.append(graph)\n",
    "\n",
    "                if farm_id not in unique_farms_idx:\n",
    "                    unique_farms_idx[farm_id] = graph\n",
    "                \n",
    "                #if aug_proof_concept_iter == 0:\n",
    "                    #aug_proof_concept[com_id] = graph\n",
    "\n",
    "                    #g = to_networkx(graph, to_undirected=True)\n",
    "                    #nx.draw(g, pos=nx.spring_layout(g), with_labels=True)\n",
    "\n",
    "                    #plt.show()\n",
    "    \n",
    "    return (graphs, unique_farms_idx, aug_proof_concept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(graph, max_depth, samples):\n",
    "    result = []\n",
    "    unused_nodes = [n for n in graph.nodes()]\n",
    "    for _ in range(samples):\n",
    "        attempts = len(unused_nodes)\n",
    "        while attempts > 0:\n",
    "            start_node = random.choice(unused_nodes)\n",
    "            unused_nodes.remove(start_node)\n",
    "            walk = [start_node]\n",
    "            visited = set()\n",
    "            for depth in range(max_depth):\n",
    "                if depth == 0:\n",
    "                    neighbors = [n for n in graph.neighbors(start_node) if n not in visited]\n",
    "                else:\n",
    "                    neighbors = [n for node in walk for n in graph.neighbors(node) if n not in visited]\n",
    "                if neighbors:\n",
    "                    walk.extend(neighbors)\n",
    "                    visited.update(neighbors)\n",
    "                else:\n",
    "                    break\n",
    "            if len(walk) > 3:\n",
    "                result.append(walk)\n",
    "                break\n",
    "            else:\n",
    "                attempts -= 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_HGNN_random_walk(sub_df_norm, masking_data, distance_txt_file, dist_threshold_choice = [20, 25, 30, 35, 40], max_depth = 2, samples = 5):\n",
    "\n",
    "    farm_ids, dates = extract_MetaData(sub_df_norm)\n",
    "\n",
    "    # list to store the graph for each farm and date\n",
    "    graphs = []\n",
    "    unique_farms_idx = {}\n",
    "    aug_proof_concept = {}\n",
    "    aug_proof_concept_iter = -1\n",
    "    nbr_total_nodes = 0\n",
    "    nbr_total_edges = 0\n",
    "    size_communities = []\n",
    "    nbr_subgraphs = 0\n",
    "\n",
    "\n",
    "    # loop through each farm and date\n",
    "    for (farm_id, date) in itertools.product(farm_ids, dates):\n",
    "\n",
    "        train_masks, valid_masks, test_masks, plant_tags = create_masks(masking_data, farm_id)\n",
    "        day_farm_data = sub_df_norm[(sub_df_norm['FarmID'] == farm_id) & (sub_df_norm['Date'] == date)]\n",
    "        if day_farm_data.size > 0:\n",
    "            day_farm_data_subset = day_farm_data.copy(deep=True)\n",
    "            aug_proof_concept_iter +=1 # only for visualisation purposes\n",
    "\n",
    "            #get edges\n",
    "            dist_threshold = random.choice(dist_threshold_choice)\n",
    "            donor, receiver = get_plant_distances(farm_id, dist_threshold, distance_txt_file, day_farm_data['Plant_ID'].tolist())\n",
    "            plant_edges = list(zip(donor, receiver))\n",
    "\n",
    "            # columns that should not be features in the graph\n",
    "            exclude_columns = ['FarmID', 'Plant_ID', 'Date', 'cote_c_carotae']\n",
    "            node_id_column = day_farm_data_subset['Plant_ID']\n",
    "            node_features = day_farm_data_subset.drop(exclude_columns, axis=1).to_dict('records')\n",
    "            \n",
    "\n",
    "            graph_nx = nx.Graph()\n",
    "            for node_id, features in zip(node_id_column, node_features):\n",
    "                graph_nx.add_node(node_id, **features)\n",
    "            \n",
    "            for edge in plant_edges:\n",
    "                graph_nx.add_edge(*edge)\n",
    "            \n",
    "\n",
    "            walks = random_walk(graph_nx, max_depth, samples)\n",
    "\n",
    "            if len(walks) == 0:\n",
    "                #nx.draw(graph_nx,pos=nx.spring_layout(graph_nx), with_labels=True, node_color='lightgray', node_size=300)\n",
    "                #plt.show()\n",
    "                #print(f'walks: {walks}')\n",
    "                continue\n",
    "\n",
    "            walks_subgraphs = split_graph_into_communities(graph_nx, walks)\n",
    "\n",
    "            #print(walks)\n",
    "            #plot_initial_graph_with_partitions(graph_nx, walks)\n",
    "            #plot_community_subgraphs(walks_subgraphs)\n",
    "            \n",
    "            for walk_id, subgraph in walks_subgraphs.items():\n",
    "                node_id_list = subgraph.nodes\n",
    "                edges_list = subgraph.edges\n",
    "\n",
    "                subgraph_df = day_farm_data.loc[day_farm_data['Plant_ID'].isin(node_id_list)]\n",
    "\n",
    "                # extract appropriate masks\n",
    "                train_masks_subgraph = [train_masks[i - 1] for i in node_id_list]\n",
    "                valid_masks_subgraph = [valid_masks[i - 1] for i in node_id_list]\n",
    "                test_masks_subgraph = [test_masks[i - 1] for i in node_id_list]\n",
    "                plant_tags_subgraph = [plant_tags[i - 1] for i in node_id_list]\n",
    "\n",
    "                # resetting node IDS\n",
    "                new_plant_id = list(range(0, len(node_id_list)))\n",
    "                new_plant_id_tuples = list(zip(node_id_list, new_plant_id))\n",
    "\n",
    "                for old_id, new_id in new_plant_id_tuples:\n",
    "                    subgraph_df.loc[subgraph_df[\"Plant_ID\"] == old_id, \"Plant_ID\"] = new_id\n",
    "\n",
    "                id_dict = dict(new_plant_id_tuples)\n",
    "                updated_edges_list = [(id_dict.get(old_id_1, old_id_1), id_dict.get(old_id_2, old_id_2)) for old_id_1, old_id_2 in edges_list]\n",
    "\n",
    "                # shared operations between augmented and non-augmented graphs\n",
    "                if len(updated_edges_list) == 0:\n",
    "                    continue # we skip graphs with no edges at all\n",
    "\n",
    "                donor, receiver = zip(*updated_edges_list)\n",
    "                donor_np = np.array(donor)\n",
    "                receiver_np = np.array(receiver)\n",
    "\n",
    "                # we need edges going both ways \n",
    "                from_plant1 = torch.tensor(donor_np, dtype = int)\n",
    "                to_plant2 = torch.tensor(receiver_np, dtype = int)\n",
    "\n",
    "                plant_edges = torch.concat((from_plant1, to_plant2)).reshape(-1, len(from_plant1)).long()\n",
    "\n",
    "                #convert class values to tensor\n",
    "                squamosa_rate_array = subgraph_df['cote_c_carotae'].to_numpy()\n",
    "                squamosa_rate_label = np.rint(squamosa_rate_array) # round up cote squamosa to nearest integer\n",
    "                squamosa_rate_label = torch.tensor(squamosa_rate_label, dtype = int)\n",
    "\n",
    "                #remove vars we dont want in the graph (the metadata)\n",
    "                day_farm_data_clean = subgraph_df.drop(['FarmID', 'Plant_ID', 'Date', 'cote_c_carotae'], axis=1)\n",
    "\n",
    "                # create plant node as tensor\n",
    "                plants_tensor = torch.tensor(np.array(day_farm_data_clean), dtype = float)\n",
    "\n",
    "                graph = Data(x=plants_tensor, edge_index=plant_edges, y=squamosa_rate_label)\n",
    "\n",
    "                transform = T.Compose([T.ToUndirected(), T.AddSelfLoops()])\n",
    "                graph = transform(graph)\n",
    "\n",
    "                graph.train_mask = torch.Tensor(train_masks_subgraph)\n",
    "                graph.val_mask = torch.Tensor(valid_masks_subgraph)\n",
    "                graph.test_mask = torch.Tensor(test_masks_subgraph)\n",
    "                graph.plant_tags = torch.Tensor(plant_tags_subgraph)\n",
    "\n",
    "                graphs.append(graph)\n",
    "\n",
    "                if farm_id not in unique_farms_idx:\n",
    "                    unique_farms_idx[farm_id] = graph\n",
    "                \n",
    "                if aug_proof_concept_iter == 0:\n",
    "                    aug_proof_concept[walk_id] = graph\n",
    "\n",
    "                    #g = to_networkx(graph, to_undirected=True)\n",
    "                    #nx.draw(g, pos=nx.spring_layout(g), with_labels=True)\n",
    "\n",
    "                    #plt.show()\n",
    "    \n",
    "    return (graphs, unique_farms_idx, aug_proof_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche par manipulation des graphes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_HGNN_aug(sub_df_norm, masking_data, distance_txt_file, dist_threshold_choice = [20, 25, 30, 35, 40], aug_multiplier = 3):\n",
    "\n",
    "    farm_ids, dates = extract_MetaData(sub_df_norm)\n",
    "\n",
    "    # list to store the graph for each farm and date\n",
    "    graphs = []\n",
    "    unique_farms_idx = {}\n",
    "    aug_proof_concept = {}\n",
    "    aug_proof_concept_iter = -1\n",
    "    nbr_total_nodes = 0\n",
    "    nbr_total_edges = 0\n",
    "\n",
    "    # loop through each farm and date\n",
    "    for (farm_id, date) in itertools.product(farm_ids, dates):\n",
    "\n",
    "        train_masks, valid_masks, test_masks, plant_tags = create_masks(masking_data, farm_id)\n",
    "        day_farm_data = sub_df_norm[(sub_df_norm['FarmID'] == farm_id) & (sub_df_norm['Date'] == date)]\n",
    "        if day_farm_data.size > 0:\n",
    "            aug_proof_concept_iter +=1 # only for visualisation purposes\n",
    "\n",
    "            for aug in range(aug_multiplier):\n",
    "\n",
    "                removed_plant_ids = []\n",
    "\n",
    "                # we select a distance threshold and extract the relevant plant combinations for the edges\n",
    "                dist_threshold = random.choice(dist_threshold_choice)\n",
    "                donor, receiver = get_plant_distances(farm_id, dist_threshold, distance_txt_file, day_farm_data['Plant_ID'].tolist())\n",
    "\n",
    "                # first iteration is not an augmentation task, we preserve as much data as possible for further alterations\n",
    "                if aug == 0:\n",
    "                    node_drop_prob = 0\n",
    "                    edge_drop_prob = 0\n",
    "                    edge_add_nbr = 0\n",
    "\n",
    "                    squamosa_rate_array = day_farm_data['cote_c_carotae'].to_numpy()\n",
    "                    squamosa_rate_label = np.rint(squamosa_rate_array) # round up cote squamosa to nearest integer\n",
    "                    #print(f'nbr label:', len(squamosa_rate_label))\n",
    "                    day_farm_data_subset = day_farm_data # for compatibility with the aug operations\n",
    "\n",
    "                    plant_ids = sub_df_norm[(sub_df_norm['FarmID'] == farm_id) & (sub_df_norm['Date'] == date)]['Plant_ID'].tolist()\n",
    "                    #print(f'nbr ids:', len(plant_ids))\n",
    "                    train_masks_subgraph = [train_masks[i - 1] for i in plant_ids]\n",
    "                    valid_masks_subgraph = [valid_masks[i - 1] for i in plant_ids]\n",
    "                    test_masks_subgraph = [test_masks[i - 1] for i in plant_ids]\n",
    "                    plant_tags_subgraph = [plant_tags[i - 1] for i in plant_ids]\n",
    "\n",
    "                    if len(plant_ids) != max(plant_ids):                      \n",
    "                        day_farm_data_subset, donor, receiver = clean_ID_sparse(day_farm_data_subset, \"Plant_ID\", donor, receiver)\n",
    "                \n",
    "                # iteration > 1 are augmentation tasks, we apply a random assortment of operations\n",
    "                else:\n",
    "                    day_farm_data_subset = day_farm_data.copy(deep=True) # we want to keep the original values so each iteration is independant\n",
    "\n",
    "                    node_drop_prob = random.choice([0, 0.1, 0.2])\n",
    "                    edge_drop_prob = random.choice([0, 0.1, 0.2])\n",
    "                    edge_add_nbr = random.choice([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "                    #node drop\n",
    "                    #using the dataframe, we remove n rows based on the node drop probability\n",
    "                    nbr_nodes_drop = int(node_drop_prob * len(day_farm_data_subset))\n",
    "                    if nbr_nodes_drop > 0:\n",
    "                        \n",
    "                        # sample the df n times and remove the picked rows\n",
    "                        day_farm_data_subset_filtered = day_farm_data_subset[~day_farm_data_subset['Plant_ID'].isin(conserved_node_ids)] # we keep 20% most connected nodes safe\n",
    "                        removed_rows = day_farm_data_subset_filtered.sample(nbr_nodes_drop)\n",
    "                        removed_plant_ids = (removed_rows['Plant_ID'].tolist())\n",
    "                        day_farm_data_subset = day_farm_data_subset.drop(removed_rows.index)\n",
    "\n",
    "                        # remove the donor-receiver pairs that include the removed nodes\n",
    "                        plant_edges = list(zip(donor, receiver))\n",
    "                        temp_edges = []\n",
    "                        for don, rec in plant_edges:\n",
    "                            if don not in removed_plant_ids and rec not in removed_plant_ids:\n",
    "                                temp_edges.append((don, rec))\n",
    "                        \n",
    "                        donor, receiver = zip(*temp_edges)\n",
    "                        \n",
    "                        squamosa_rate_array = day_farm_data_subset['cote_c_carotae'].to_numpy()\n",
    "                        squamosa_rate_label = np.rint(squamosa_rate_array) # round up cote squamosa to nearest integer\n",
    "                    \n",
    "                    else:\n",
    "                        day_farm_data_subset = day_farm_data\n",
    "                        squamosa_rate_array = day_farm_data_subset['cote_c_carotae'].to_numpy()\n",
    "                        squamosa_rate_label = np.rint(squamosa_rate_array) # round up cote squamosa to nearest integer\n",
    "\n",
    "\n",
    "                    #now we have to re_ID come of the plants so that there is no indexing errors down the line\n",
    "                    day_farm_data_subset, donor, receiver = clean_ID_sparse(day_farm_data_subset, \"Plant_ID\", donor, receiver)\n",
    "\n",
    "                    node_id_list = day_farm_data_subset[\"Plant_ID\"].tolist()\n",
    "                    \n",
    "                    # extract appropriate masks\n",
    "                    train_masks_subgraph = [train_masks[i - 1] for i in node_id_list]\n",
    "                    valid_masks_subgraph = [valid_masks[i - 1] for i in node_id_list]\n",
    "                    test_masks_subgraph = [test_masks[i - 1] for i in node_id_list]\n",
    "                    plant_tags_subgraph = [plant_tags[i - 1] for i in node_id_list]\n",
    "\n",
    "                    #edge drop\n",
    "                    nbr_edges_drop = int(edge_drop_prob * len(donor))\n",
    "                    plant_edges = list(zip(donor, receiver))\n",
    "\n",
    "                    if nbr_edges_drop > 0:\n",
    "                        random.shuffle(plant_edges)\n",
    "                        plant_edges = plant_edges[:len(plant_edges)-nbr_edges_drop] # we remove n random edge pairs\n",
    "\n",
    "                    # edge add\n",
    "                    if edge_add_nbr > 0:\n",
    "                        patience = 10\n",
    "                        remaining_edges = edge_add_nbr\n",
    "                        node_id_list = day_farm_data_subset[\"Plant_ID\"].tolist()\n",
    "                        while patience > 0 and remaining_edges > 0:\n",
    "                            new_donor = random.choice(node_id_list)\n",
    "                            new_receiver = random.choice(node_id_list)\n",
    "                            temp_edge = (new_donor, new_receiver)\n",
    "                            if temp_edge in plant_edges or new_donor == new_receiver:\n",
    "                                patience -= 1\n",
    "                            else:\n",
    "                                plant_edges.append(temp_edge)\n",
    "                                remaining_edges -= 1\n",
    "                                patience = 10\n",
    "                    \n",
    "                    donor, receiver = zip(*plant_edges)\n",
    "\n",
    "                # shared operations between augmented and non-augmented graphs\n",
    "\n",
    "                donor_np = np.array(donor)\n",
    "                receiver_np = np.array(receiver)\n",
    "\n",
    "                # we need edges going both ways \n",
    "                from_plant1 = torch.tensor(donor_np, dtype = int)\n",
    "                to_plant2 = torch.tensor(receiver_np, dtype = int)\n",
    "\n",
    "                plant_edges = torch.concat((from_plant1, to_plant2)).reshape(-1, len(from_plant1)).long()\n",
    "\n",
    "                #remove vars we dont want in the graph (the metadata)\n",
    "                day_farm_data_clean = day_farm_data_subset.drop(['FarmID', 'Plant_ID', 'Date', 'cote_c_carotae'], axis=1)\n",
    "\n",
    "                #convert class values to tensor\n",
    "                squamosa_rate_label = torch.tensor(squamosa_rate_label, dtype = int)\n",
    "\n",
    "                # create plant node as tensor\n",
    "                plants_tensor = torch.tensor(np.array(day_farm_data_clean), dtype = float)\n",
    "\n",
    "                graph = Data(x=plants_tensor, edge_index=plant_edges, y=squamosa_rate_label)\n",
    "\n",
    "                transform = T.Compose([T.ToUndirected(), T.AddSelfLoops()])\n",
    "                graph = transform(graph)\n",
    "\n",
    "                graph.train_mask = torch.Tensor(train_masks_subgraph)\n",
    "                graph.val_mask = torch.Tensor(valid_masks_subgraph)\n",
    "                graph.test_mask = torch.Tensor(test_masks_subgraph)\n",
    "                graph.plant_tags = torch.Tensor(plant_tags_subgraph)\n",
    "                \n",
    "                nbr_total_nodes += len(plants_tensor)\n",
    "                nbr_total_edges += len(plant_edges[0])\n",
    "                graphs.append(graph)\n",
    "\n",
    "                if aug == 0:\n",
    "                    #print(f'graph.edge_index[0]:', graph.edge_index[0])\n",
    "                    #print(f'graph.edge_index[1]:', graph.edge_index[1])\n",
    "                    #print(f'graph.num_nodes:', graph.num_nodes)\n",
    "                    #print(f'plants_tensor:', plants_tensor)\n",
    "                    #print(f'len(plants_tensor):', len(plants_tensor))\n",
    "                    #print('-------')\n",
    "                    degrees = degree(graph.edge_index[0], num_nodes=graph.num_nodes, dtype=torch.float)\n",
    "                    node_degrees = list(enumerate(degrees.tolist()))\n",
    "                    # Sort nodes by degree in descending order\n",
    "                    sorted_nodes = sorted(node_degrees, key=lambda x: x[1], reverse=True)\n",
    "                    # Extract node IDs from sorted list\n",
    "                    sorted_node_ids = [node_id for node_id, _ in sorted_nodes]\n",
    "                    conserved_node_ids = sorted_node_ids[:round(0.2*len(sorted_node_ids))]\n",
    "\n",
    "                # used to save a graph of each farm for visualisation purposes only\n",
    "                if farm_id not in unique_farms_idx:\n",
    "                    unique_farms_idx[farm_id] = graph\n",
    "                \n",
    "                if aug_proof_concept_iter == 0:\n",
    "                    aug_proof_concept[aug] = graph\n",
    "\n",
    "    return (graphs, unique_farms_idx, aug_proof_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methode sans augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(masking_data, farm_id):\n",
    "\n",
    "    filtered_lists = [[y for x, y in sublist if x == farm_id] for sublist in masking_data]\n",
    "\n",
    "    total_len = sum(len(inner_list) for inner_list in filtered_lists)\n",
    "    train_masks = [0] * total_len\n",
    "    valid_masks = [0] * total_len\n",
    "    test_masks = [0] * total_len\n",
    "\n",
    "    train_masks = [1 if i + 1 in filtered_lists[0] else 0 for i in range(total_len)]\n",
    "    valid_masks = [1 if i + 1 in filtered_lists[1] else 0 for i in range(total_len)]\n",
    "    test_masks = [1 if i + 1 in filtered_lists[2] else 0 for i in range(total_len)]\n",
    "    plant_tags = [farm_id*100 + i+1 for i in range(total_len)]\n",
    "    \n",
    "    return train_masks, valid_masks, test_masks, plant_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_HGNN(sub_df_norm, masking_data, distance_txt_file, dist_threshold, class_level):\n",
    "\n",
    "  farm_ids, dates = extract_MetaData(sub_df_norm)\n",
    "\n",
    "  # list to store the graph for each farm and date\n",
    "  graphs = []\n",
    "  unique_farms_idx = {}\n",
    "  i = 0\n",
    "  nbr_total_nodes = 0\n",
    "  nbr_total_edges = 0\n",
    "\n",
    "\n",
    "  # loop through each farm and date\n",
    "  for (farm_id, date) in itertools.product(farm_ids, dates):\n",
    "    \n",
    "    train_masks, valid_masks, test_masks, plant_tags = create_masks(masking_data, farm_id)\n",
    "    plant_ids = sub_df_norm[(sub_df_norm['FarmID'] == farm_id) & (sub_df_norm['Date'] == date)]['Plant_ID'].tolist()\n",
    "    train_masks_subgraph = [train_masks[i - 1] for i in plant_ids]\n",
    "    valid_masks_subgraph = [valid_masks[i - 1] for i in plant_ids]\n",
    "    test_masks_subgraph = [test_masks[i - 1] for i in plant_ids]\n",
    "    plant_tags_subgraph = [plant_tags[i - 1] for i in plant_ids]\n",
    "    \n",
    "    day_farm_data = sub_df_norm[(sub_df_norm['FarmID'] == farm_id) & (sub_df_norm['Date'] == date)]\n",
    "    j = 0\n",
    "    if day_farm_data.size > 0:\n",
    "      # extract class values\n",
    "      if class_level == 'graph':\n",
    "        squamosa_rate_array = day_farm_data['cote_c_carotae'].to_numpy()\n",
    "        squamosa_rate_max = max(squamosa_rate_array)\n",
    "        squamosa_rate_label = np.rint(squamosa_rate_max) # round up cote squamosa to nearest integer\n",
    "\n",
    "      elif class_level == 'node':\n",
    "        squamosa_rate_array = day_farm_data['cote_c_carotae'].to_numpy()\n",
    "        squamosa_rate_label = np.rint(squamosa_rate_array) # round up cote squamosa to nearest integer\n",
    "      else:\n",
    "        print(f'class_level must be \"graph\", not {class_level}.')\n",
    "      \n",
    "      # create edge: edge plant to plant\n",
    "      donor, receiver = get_plant_distances(farm_id, dist_threshold, distance_txt_file, day_farm_data['Plant_ID'].tolist())\n",
    "\n",
    "      if len(plant_ids) != max(plant_ids):\n",
    "        day_farm_data, donor, receiver = clean_ID_sparse(day_farm_data, \"Plant_ID\", donor, receiver)\n",
    "\n",
    "\n",
    "      donor_np = np.array(donor)\n",
    "      receiver_np = np.array(receiver)\n",
    "      # we need edges going both ways \n",
    "      from_plant1 = torch.tensor(donor_np, dtype = int)\n",
    "      to_plant2 = torch.tensor(receiver_np, dtype = int)\n",
    "\n",
    "      plant_edges = torch.concat((from_plant1, to_plant2)).reshape(-1, len(from_plant1)).long()\n",
    "\n",
    "\n",
    "      #remove vars we dont want in the graph (the metadata)\n",
    "      day_farm_data = day_farm_data.drop(['FarmID', 'Plant_ID', 'Date', 'cote_c_carotae'], axis=1)\n",
    "\n",
    "      #convert class values to tensor\n",
    "      squamosa_rate_label = torch.tensor(squamosa_rate_label, dtype = int)\n",
    "\n",
    "      # create plant node as tensor\n",
    "      plants_tensor = torch.tensor(np.array(day_farm_data), dtype = float)\n",
    "\n",
    "      graph = Data(x=plants_tensor, edge_index=plant_edges, y=squamosa_rate_label)\n",
    "\n",
    "      transform = T.Compose([T.ToUndirected(), T.AddSelfLoops()])\n",
    "      graph = transform(graph)\n",
    "\n",
    "      graph.train_mask = torch.Tensor(train_masks_subgraph)\n",
    "      graph.val_mask = torch.Tensor(valid_masks_subgraph)\n",
    "      graph.test_mask = torch.Tensor(test_masks_subgraph)\n",
    "      graph.plant_tags = torch.Tensor(plant_tags_subgraph)\n",
    "      graph.graph_date = torch.full((len(plant_tags_subgraph),), date)\n",
    "      \n",
    "      i+= 1\n",
    "      nbr_total_nodes += len(plants_tensor)\n",
    "      nbr_total_edges += len(plant_edges[0])\n",
    "      graphs.append(graph)\n",
    "\n",
    "      if (len(train_masks_subgraph)) != len(squamosa_rate_array):\n",
    "        print('error: training mask not right size')\n",
    "\n",
    "      if farm_id not in unique_farms_idx:\n",
    "        unique_farms_idx[farm_id] = graph\n",
    "\n",
    "  return (graphs, unique_farms_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele + hypertuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(graphs, train_ratio = 0.75, val_ratio = 0.15):\n",
    "    random.shuffle(graphs)\n",
    "    train_split = int(len(graphs) * train_ratio)\n",
    "    valid_split = int(len(graphs) * val_ratio)\n",
    "    \n",
    "    train_data = graphs[:train_split]\n",
    "    val_data = graphs[train_split:train_split+valid_split]\n",
    "    test_data = graphs[train_split+valid_split:]\n",
    "\n",
    "\n",
    "    #training_data, test_data = train_test_split(graphs, test_size=0.1, random_state=42)\n",
    "    #train_data, val_data = train_test_split(training_data, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=4, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=4, shuffle=True)\n",
    "\n",
    "    return (train_loader, val_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_class_samples(loader, nbr_classes):\n",
    "    class_counts = torch.zeros(nbr_classes, dtype=torch.long)\n",
    "    for data in loader:\n",
    "        class_counts += torch.bincount(data.y, minlength=nbr_classes)\n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_distribution(distribution, percentage_threshold=0.1):\n",
    "    class_count = list(distribution)\n",
    "\n",
    "    smallest_class = min(class_count)\n",
    "\n",
    "    # Calculate the expected minimum and maximum class count\n",
    "    max_class_count = (1 + percentage_threshold) * smallest_class\n",
    "\n",
    "    # Check if the distribution is within the specified percentage\n",
    "    within_threshold = all(count <= max_class_count for count in class_count)\n",
    "\n",
    "    return within_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logique d'execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../../Output/Trained_models\"):\n",
    "      \n",
    "    # if the demo_folder directory is not present \n",
    "    # then create it.\n",
    "    os.makedirs(\"../../Output/Trained_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_next_dataset_folder(directory_path):\n",
    "\n",
    "    folders = [f for f in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, f))]\n",
    "\n",
    "    max_number = -1\n",
    "\n",
    "    # Iterate through the folders\n",
    "    for folder in folders:\n",
    "         if folder.startswith('created_models_'):\n",
    "            try:\n",
    "                number = int(folder.split('_')[-1])\n",
    "                max_number = max(max_number, number)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    os.makedirs(f\"created_models_{max_number + 1}\")\n",
    "    print(f\"save location: created_models_{max_number + 1}\")\n",
    "    return (f'created_models_{max_number + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pbar.close() \n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Work\\\\UQAM\\\\Doctorat\\\\Projets\\\\oignion_GNN\\\\cultures_GNN\\\\carotte\\\\Automne_2023\\\\Script\\\\graph_masking'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_masking_data(json_file):\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        loaded_masking_data = json.load(json_file)\n",
    "    \n",
    "    return(loaded_masking_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save location: created_models_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing base - no_aug @ percentage 5.0%:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing base - no_aug @ percentage 20.999999999999996%: 100%|██████████| 1/1 [00:18<00:00, 18.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved base - no_aug with train: [151, 129], valid:[27, 28], test:[35, 29], perc: 0.20999999999999996, num_graphs: 16, num nodes: 399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "### setup run parameters ###\n",
    "############################\n",
    "param_dict = dict()\n",
    "nbr_classes = 2\n",
    "\n",
    "datasets_to_run = ['base', 'Botcast']\n",
    "augmentation = ['drop_add_3','drop_add_5', \n",
    "                'no_aug', \n",
    "                'louvain', \n",
    "                'sliding_window_dist_20_40_step_1','sliding_window_dist_20_40_step_2', \n",
    "                'random_walk_depth_1_sample_10', 'random_walk_depth_2_sample_10',]\n",
    "\n",
    "dataset_store_path = create_next_dataset_folder('../graph_masking/')\n",
    "masking_data = load_masking_data('graph_masking_carrot.json')\n",
    "\n",
    "#######################\n",
    "### start main loop ###\n",
    "#######################\n",
    "\n",
    "with tqdm(total=len(datasets_to_run)*len(augmentation)) as pbar:\n",
    "\n",
    "    for filename in (os.listdir(directory)):\n",
    "        vars = ['FarmID', 'Plant_ID', 'Date','cote_c_carotae']\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip(): # remove empty lines\n",
    "                        if not line.startswith('->'): # remove df code\n",
    "                            vars.append(line.strip())\n",
    "            row_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            #print(f'Starting dataset : {row_name} of shape {extract_sub_df(combined_df, vars).shape} ')\n",
    "            \n",
    "            if row_name in datasets_to_run:\n",
    "                sub_df = extract_sub_df(combined_df, vars)\n",
    "                sub_df_norm = normalize(sub_df, exception_list=['FarmID', 'Plant_ID', 'Date', 'cote_c_carotae'])\n",
    "\n",
    "                for aug in augmentation:\n",
    "                    balanced = False\n",
    "                    iter = 0\n",
    "                    perc = 0.05\n",
    "                    while not balanced:\n",
    "                        pbar.set_description(f\"Processing {row_name} - {aug} @ percentage {perc*100}%\")\n",
    "                        if aug == 'no_aug':\n",
    "                            graphs, unique_farms_idx = build_HGNN(sub_df_norm, masking_data, distance_txt_file, 25, 'node')\n",
    "                            parameters = f'dist-{25}'\n",
    "                        elif aug == 'drop_add_3':\n",
    "                            graphs, unique_farms_idx, aug_proof_concept = build_HGNN_aug(sub_df_norm, masking_data, distance_txt_file, aug_multiplier=3)\n",
    "                            parameters = f'aug_mult-{3}'\n",
    "                        elif aug == 'drop_add_5':\n",
    "                            graphs, unique_farms_idx, aug_proof_concept = build_HGNN_aug(sub_df_norm, masking_data, distance_txt_file, aug_multiplier=3)\n",
    "                            parameters = f'aug_mult-{5}'\n",
    "                        elif aug == 'louvain':\n",
    "                            graphs, unique_farms_idx, aug_proof_concept = build_HGNN_louvain(sub_df_norm, masking_data, distance_txt_file)\n",
    "                            parameters = ''\n",
    "                        elif aug == 'sliding_window_dist_20_40_step_1':\n",
    "                            graphs, unique_farms_idx, aug_proof_concept = build_HGNN_aug_sliding_threshold(sub_df_norm, masking_data, distance_txt_file, minimum_distance=20, maximum_distance=40, step=2)\n",
    "                            parameters = f'min_dist-{20}_max_dist-{40}_step-{1}'\n",
    "                        elif aug == 'sliding_window_dist_20_40_step_2':\n",
    "                            graphs, unique_farms_idx, aug_proof_concept = build_HGNN_aug_sliding_threshold(sub_df_norm, masking_data, distance_txt_file, minimum_distance=20, maximum_distance=40, step=2)\n",
    "                            parameters = f'min_dist-{20}_max_dist-{40}_step-{2}'\n",
    "                        elif aug == 'random_walk_depth_1_sample_10':\n",
    "                            graphs, unique_farms_idx, aug_proof_concept = build_HGNN_random_walk(sub_df_norm, masking_data, distance_txt_file, max_depth=1, samples=10)\n",
    "                            parameters = f'max_depth-{1}_samples-{10}'\n",
    "                        elif aug == 'random_walk_depth_2_sample_10':\n",
    "                            graphs, unique_farms_idx, aug_proof_concept = build_HGNN_random_walk(sub_df_norm, masking_data, distance_txt_file, max_depth=1, samples=10)\n",
    "                            parameters = f'max_depth-{2}_samples-{10}'\n",
    "                        else:\n",
    "                            print(f'{aug} parameters not found in the if/elif selection block. skipping.')\n",
    "\n",
    "                        #train_loader, val_loader, test_loader = get_dataloaders(graphs, train_ratio=0.75, val_ratio=0.15)\n",
    "\n",
    "                        #train_counts = count_class_samples(train_loader, nbr_classes)\n",
    "                        #val_counts = count_class_samples(val_loader, nbr_classes)\n",
    "                        #test_counts = count_class_samples(test_loader, nbr_classes)\n",
    "\n",
    "                        graphs_train_class_counts = [0, 0]\n",
    "                        graphs_val_class_counts = [0, 0]\n",
    "                        graphs_test_class_counts = [0, 0]\n",
    "                        num_nodes = 0\n",
    "                        for idx, graph in enumerate(graphs):\n",
    "                            \n",
    "                            # Access node labels\n",
    "                            node_labels = graph.y\n",
    "                            num_nodes += len(node_labels)\n",
    "                            # Convert masks to the appropriate data type\n",
    "                            train_mask = graph.train_mask.bool()\n",
    "                            val_mask = graph.val_mask.bool()\n",
    "                            test_mask = graph.test_mask.bool()\n",
    "\n",
    "                            if len(train_mask) != len(node_labels):\n",
    "                                print(aug)\n",
    "                                print(graph)\n",
    "                            \n",
    "                            # Get unique classes present in the graph\n",
    "                            unique_classes = torch.unique(node_labels)\n",
    "                            \n",
    "                            # Create count lists with zeros for both classes\n",
    "                            train_class_counts = [0, 0]\n",
    "                            val_class_counts = [0, 0]\n",
    "                            test_class_counts = [0, 0]\n",
    "                            \n",
    "                            # Calculate counts if classes are present\n",
    "                            if len(unique_classes) > 0:\n",
    "                                counts = torch.bincount(node_labels[train_mask], minlength=len(unique_classes))\n",
    "                                train_class_counts[:len(counts)] = counts.tolist()\n",
    "                                graphs_train_class_counts = [x + y for x, y in zip(graphs_train_class_counts, train_class_counts)]\n",
    "                                \n",
    "                                counts = torch.bincount(node_labels[val_mask], minlength=len(unique_classes))\n",
    "                                val_class_counts[:len(counts)] = counts.tolist()\n",
    "                                graphs_val_class_counts = [x + y for x, y in zip(graphs_val_class_counts, val_class_counts)]\n",
    "                                \n",
    "                                counts = torch.bincount(node_labels[test_mask], minlength=len(unique_classes))\n",
    "                                test_class_counts[:len(counts)] = counts.tolist()\n",
    "                                graphs_test_class_counts = [x + y for x, y in zip(graphs_test_class_counts, test_class_counts)]\n",
    "                            \n",
    "                            #print(f\"Training Set - Class 0: {train_class_counts[0]}, Class 1: {train_class_counts[1]}\")\n",
    "                            #print(f\"Validation Set - Class 0: {val_class_counts[0]}, Class 1: {val_class_counts[1]}\")\n",
    "                            #print(f\"Test Set - Class 0: {test_class_counts[0]}, Class 1: {test_class_counts[1]}\")\n",
    "                            #print(\"\\n\")\n",
    "                            \n",
    "\n",
    "                        if (check_class_distribution(graphs_train_class_counts, percentage_threshold= perc) and \n",
    "                            check_class_distribution(graphs_val_class_counts, percentage_threshold= perc) and \n",
    "                            check_class_distribution(graphs_test_class_counts, percentage_threshold= perc)):\n",
    "                            balanced = True\n",
    "                        \n",
    "                        iter +=1\n",
    "                        if iter == 50:\n",
    "                            perc += 0.02\n",
    "                            iter = 0\n",
    "                    \n",
    "                    if len(graphs) < 10:\n",
    "                        batch_size = 2\n",
    "                    elif 10 <= len(graphs) < 50:\n",
    "                        batch_size = 4\n",
    "                    elif 50 <= len(graphs) < 150:\n",
    "                        batch_size = 8\n",
    "                    elif 150 <= len(graphs) < 500:\n",
    "                        batch_size = 16\n",
    "                    elif 150 <= len(graphs) < 500:\n",
    "                        batch_size = 32\n",
    "                    elif 500 <= len(graphs) < 5000:\n",
    "                        batch_size = 64\n",
    "                    elif len(graphs) >= 5000:\n",
    "                        batch_size = 128\n",
    "                    else:\n",
    "                        print('error batch size not in predicted range')\n",
    "                    \n",
    "                    random.shuffle(graphs)\n",
    "                    graphs_loader = DataLoader(graphs, batch_size=batch_size, shuffle=True)\n",
    "                    \n",
    "                    save_folder = f'{dataset_store_path}/{row_name}_{aug}_{parameters}'\n",
    "                    os.makedirs(save_folder, exist_ok=True)\n",
    "                    torch.save(graphs_loader, f'{save_folder}/graphs_dataset.pth')\n",
    "\n",
    "                    print(f'saved {row_name} - {aug} with train: {graphs_train_class_counts}, valid:{graphs_val_class_counts}, test:{graphs_test_class_counts}, perc: {perc}, num_graphs: {len(graphs)}, num nodes: {num_nodes}')\n",
    "                    pbar.update(1)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[25, 131], edge_index=[2, 71], y=[25], train_mask=[25], val_mask=[25], test_mask=[25], plant_tags=[25], graph_date=[25])\n",
      "Data(x=[25, 131], edge_index=[2, 85], y=[25], train_mask=[25], val_mask=[25], test_mask=[25], plant_tags=[25], graph_date=[25])\n",
      "Data(x=[25, 131], edge_index=[2, 71], y=[25], train_mask=[25], val_mask=[25], test_mask=[25], plant_tags=[25], graph_date=[25])\n",
      "Data(x=[24, 131], edge_index=[2, 66], y=[24], train_mask=[24], val_mask=[24], test_mask=[24], plant_tags=[24], graph_date=[24])\n"
     ]
    }
   ],
   "source": [
    "for data in graphs_loader:\n",
    "    print(data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
