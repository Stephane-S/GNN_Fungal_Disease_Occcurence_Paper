{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from tqdm import tqdm_notebook, trange\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import pathlib\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import dropout_edge, mask_feature\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# visual\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#local\n",
    "from GCN_model.Standard_GCN import GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuda preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "    #device = torch.device('cuda')\n",
    "    #CUDA_LAUNCH_BLOCKING=1\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "#device = torch.device('cpu')\n",
    "print(f'running with {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuning + training logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outptut_variance_compute(model, graphs_dataset):\n",
    "    cumul_variance = 0\n",
    "    for batchdata in graphs_dataset:\n",
    "            batchdata_list = batchdata.to_data_list()\n",
    "            for data in batchdata_list:\n",
    "                data.to(device)\n",
    "                output = model(data.x, data.edge_index)\n",
    "                pred = output.argmax(dim=1)\n",
    "\n",
    "                cumul_variance += np.var(pred.detach().cpu().numpy())\n",
    "                \n",
    "    return str(cumul_variance)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(graphs_dataset, num_classes, params):\n",
    "\n",
    "    best_accuracy = -1\n",
    "    best_F1 = -1\n",
    "    best_params = None\n",
    "    best_model_dict = None\n",
    "    best_cm = None\n",
    "    best_f1 = None\n",
    "    best_cm = None\n",
    "    best_roc_auc = -1\n",
    "    len_graph_dataset = len(graphs_dataset)\n",
    "    \n",
    "    # creating weights\n",
    "    labels = []\n",
    "    for data in graphs_dataset:\n",
    "        labels.extend(data.y.numpy())\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    #### if weights desired ####\n",
    "    #class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "    #class_weights = torch.tensor(class_weights, dtype=torch.float64).to(device)\n",
    "    \n",
    "    for batch in graphs_dataset:\n",
    "        first_batch = batch\n",
    "        first_graph = first_batch[0]\n",
    "        break\n",
    "\n",
    "    val_losses_list = []\n",
    "    train_losses_list = []\n",
    "    val_acc_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    batch_size, max_epochs, num_layers, hidden_channels, optimizer_type, learning_rate, dropout_rate, fc_layer_channels = params\n",
    "\n",
    "    model = GCN(num_features=first_graph.num_features, hidden_channels=hidden_channels, num_classes=num_classes, num_layers=num_layers, dropout_rate=dropout_rate, fc_layer_channels=fc_layer_channels).double().to(device)\n",
    "\n",
    "    if optimizer_type == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "\n",
    "    best_model_state_dict = None\n",
    "    best_validation_acc = -1\n",
    "    best_epoch = -1\n",
    "    #early_stopper = EarlyStopper(patience=100, min_delta=0)\n",
    "    \n",
    "    #for epoch in range(max_epochs):\n",
    "    for epoch in trange(max_epochs, leave=False):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_nodes = 0\n",
    "\n",
    "        for batch in graphs_dataset:\n",
    "            batch.train_mask = batch.train_mask.type(torch.bool)\n",
    "\n",
    "            data = batch.to(device)\n",
    "            \n",
    "            #this does xero_grad() more efficiently\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "            output = model(data.x, data.edge_index)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            #correct = pred.eq(data.y).sum().item()\n",
    "            correct = (pred[data.train_mask] == data.y[data.train_mask]).sum()\n",
    "            total_correct += correct\n",
    "            total_nodes += int(torch.sum(data.train_mask))\n",
    "\n",
    "            loss = F.cross_entropy(output[data.train_mask], data.y[data.train_mask])#weight=class_weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_losses_list.append(total_loss / len_graph_dataset)\n",
    "        train_acc = total_correct / total_nodes\n",
    "        train_acc_list.append(train_acc.detach().cpu().numpy())\n",
    "\n",
    "        # Validation after each epoch on the validation set (val_mask)\n",
    "        model.eval()\n",
    "        total_correct_val = 0\n",
    "        total_nodes_val = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in graphs_dataset:\n",
    "                batch.val_mask = batch.val_mask.type(torch.bool)\n",
    "                data = batch.to(device)\n",
    "\n",
    "                output = model(data.x, data.edge_index)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "\n",
    "                total_correct_val += correct\n",
    "                total_nodes_val += int(torch.sum(data.val_mask))\n",
    "                val_loss = F.cross_entropy(output[data.val_mask], data.y[data.val_mask])#weight=class_weights\n",
    "                if not np.isnan(val_loss.item()):\n",
    "                    total_val_loss += val_loss.item()\n",
    "        \n",
    "        if np.isnan(total_val_loss / len_graph_dataset):\n",
    "            print(f'NaN detected: {total_val_loss} and {len_graph_dataset}')\n",
    "        val_losses_list.append(total_val_loss / len_graph_dataset)\n",
    "        val_acc = total_correct_val / total_nodes_val\n",
    "        val_acc_list.append(val_acc.detach().cpu().numpy())\n",
    "\n",
    "        #if early_stopper.early_stop(total_val_loss / len_graph_dataset):             \n",
    "            #break\n",
    "        \n",
    "        if val_acc > best_validation_acc:\n",
    "                best_validation_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model_state_dict = model.state_dict().copy()\n",
    "        \n",
    "    # Use the test_mask for final evaluation\n",
    "    if best_model_state_dict is not None:\n",
    "        # Load the best model state back into the model\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_nodes = 0\n",
    "        true_labels = list()\n",
    "        predicted_labels = list()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in graphs_dataset:\n",
    "                batch.test_mask = batch.test_mask.type(torch.bool)\n",
    "                data = batch.to(device)\n",
    "\n",
    "                output = model(data.x, data.edge_index)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "                total_correct += correct\n",
    "                total_nodes += int(torch.sum(data.test_mask))\n",
    "                true_labels.extend(data.y[data.test_mask].cpu().numpy())\n",
    "                predicted_labels.extend(pred[data.test_mask].cpu().numpy())\n",
    "\n",
    "\n",
    "        test_acc_best = total_correct / total_nodes\n",
    "        test_acc_list.append(test_acc_best.detach().cpu().numpy())\n",
    "        \n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "        roc_auc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "        cumul_variance = outptut_variance_compute(model, graphs_dataset)\n",
    "\n",
    "    if test_acc_best > best_accuracy:\n",
    "        best_accuracy = test_acc_best\n",
    "        best_params = params\n",
    "        best_cm = cm\n",
    "        best_roc_auc = roc_auc\n",
    "        best_model_dict = model.state_dict().copy()\n",
    "\n",
    "    window_size = 10\n",
    "    val_losses_list_mv = np.convolve(val_losses_list, np.ones(window_size) / window_size, mode='valid').tolist()\n",
    "    train_acc_losses_list_mv = np.convolve(train_losses_list, np.ones(window_size) / window_size, mode='valid').tolist()\n",
    "    train_acc_list_mv = np.convolve(train_acc_list, np.ones(window_size) / window_size, mode='valid').tolist()\n",
    "    \n",
    "\n",
    "    plot_dict = {'val_losses_list': val_losses_list_mv,\n",
    "                    'train_losses_list': train_acc_losses_list_mv,\n",
    "                    'val_acc_list': val_acc_list,\n",
    "                    'train_acc_list': train_acc_list_mv,\n",
    "                    'test_acc_list': test_acc_list}\n",
    "\n",
    "    return best_accuracy, best_params, best_model_dict, best_cm, best_roc_auc, plot_dict, cumul_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results saving tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir_if_absent(path):\n",
    "    if not os.path.exists(path):      \n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes sure the output folder that contains every result subfolders exists at ../Output/Trained_models \n",
    "\n",
    "if not os.path.exists(\"../../Output/Trained_models_masking\"):      \n",
    "    # if the demo_folder directory is not present \n",
    "    # then create it.\n",
    "    os.makedirs(\"../../Output/Trained_models_masking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a result sub-folder for this run\n",
    "def create_next_model_folder(directory_path):\n",
    "\n",
    "    folders = [f for f in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, f))]\n",
    "\n",
    "    largest_number = -1\n",
    "\n",
    "    # Iterate through the folders\n",
    "    for folder in folders:\n",
    "        # Use regular expression to extract the number at the end of the folder name\n",
    "        match = re.search(r'\\d+$', folder)\n",
    "        if match:\n",
    "            number = int(match.group())\n",
    "            if number > largest_number:\n",
    "                largest_number = number\n",
    "\n",
    "    os.makedirs(f\"../../Output/Trained_models_masking/experiment_{largest_number+1}\")\n",
    "    print(f'created folder ../../Output/Trained_models_masking/experiment_{largest_number+1}')\n",
    "    return (f'../../Output/Trained_models_masking/experiment_{largest_number+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_df(directory, runs):\n",
    "    subdirectories = [subdir for subdir in os.listdir(directory)]\n",
    "\n",
    "    row_names = list(set(subdirectories))\n",
    "\n",
    "    col_names = [i for i in range(runs)]\n",
    "\n",
    "    df = pd.DataFrame(columns=col_names, index=row_names)\n",
    "\n",
    "    return (df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_datasets(project):\n",
    "    dataset_count = 0\n",
    "    for dataset in project:\n",
    "        folder_list = os.listdir(dataset)\n",
    "        dataset_count += len(folder_list)\n",
    "    \n",
    "    return dataset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a row exist in the df that already cover a specific instance\n",
    "def row_exists(df, new_row):\n",
    "\n",
    "    return ((df['crop'] == str(new_row['crop'])) \n",
    "            & (df['dataset'] == str(new_row['dataset'])) \n",
    "            & (df['augmentation'] == str(new_row['augmentation'])) \n",
    "            & (df['run_number'] == int(new_row['run_number']))\n",
    "            & (df['loop'] == int(new_row['loop']))\n",
    "            & (df['parameters'] == str(new_row['parameters']))).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row(df, new_row):\n",
    "    if not row_exists(df, new_row):\n",
    "        # Get the intersection of keys in new_row and columns of DataFrame\n",
    "        valid_keys = set(new_row.keys()) & set(df.columns)\n",
    "        \n",
    "        # Create a new dictionary with only valid keys\n",
    "        valid_new_row = {key: new_row[key] for key in valid_keys}\n",
    "\n",
    "        valid_new_row_df = pd.DataFrame([valid_new_row])\n",
    "\n",
    "        # Concatenate df and valid_new_row_df\n",
    "        df = pd.concat([df, valid_new_row_df], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|output\n",
    "-|analysis_i\n",
    "--|datasets_dir\n",
    "---|dataset_folder\n",
    "----|params\n",
    "-----|models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_config_file(path, name, config_dict):\n",
    "                                \n",
    "    file_path = pathlib.Path(path) / name\n",
    "    with file_path.open('w') as f:\n",
    "        json.dump(config_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_config(**kwargs):\n",
    "    config_dict = {\n",
    "        **kwargs\n",
    "    }\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_result_dir_and_files(config_dict, col_list):\n",
    "    # Create the analysis subfolder that will contains the trained models and accuracy results\n",
    "    if config_dict[\"restart\"] is None:\n",
    "        trained_models_path = create_next_model_folder('../../Output/Trained_models_masking')\n",
    "        create_analysis_config_file(trained_models_path, \"analysis_config.json\", config_dict)\n",
    "    else:\n",
    "        trained_models_path = config_dict[\"restart\"]\n",
    "\n",
    "\n",
    "\n",
    "    if os.path.exists(f'{trained_models_path}/summary.csv'):\n",
    "        result_df = pd.read_csv(f'{trained_models_path}/summary.csv', header=0, index_col=0)\n",
    "\n",
    "    else:\n",
    "        result_df_cols = ['crop', 'dataset', 'augmentation', 'loop', 'run_number', 'parameters', 'accuracy', 'TP', 'FP', 'FN', 'TN', 'ROC-AUC', 'precision', 'recall', 'F1-score', 'epochs', 'cumul_variance']\n",
    "        result_df_cols += list(col_list)\n",
    "        result_df = pd.DataFrame(columns=result_df_cols)\n",
    "    \n",
    "    return trained_models_path, result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subplot(axes, kfold_counter, plot_dict, best_accuracy, cm):\n",
    "    axes[kfold_counter, 0].plot(range(len(plot_dict['train_losses_list'])), plot_dict['train_losses_list'], label='Training Loss')\n",
    "    axes[kfold_counter, 0].plot(range(len(plot_dict['val_losses_list'])), plot_dict['val_losses_list'], label='Validation Loss')\n",
    "    axes[kfold_counter, 0].set_xlabel('Epochs')\n",
    "    axes[kfold_counter, 0].set_ylabel('Loss')\n",
    "    axes[kfold_counter, 0].legend()\n",
    "\n",
    "    axes[kfold_counter, 1].plot(range(len(plot_dict['train_acc_list'])), plot_dict['train_acc_list'], label='Training Accuracy')\n",
    "    axes[kfold_counter, 1].plot(range(len(plot_dict['val_acc_list'])), plot_dict['val_acc_list'], label='Validation Accuracy')\n",
    "    axes[kfold_counter, 1].plot(range(len(plot_dict['test_acc_list'])), plot_dict['test_acc_list'], label='Test Accuracy')\n",
    "    axes[kfold_counter, 1].axhline(y = round(float(best_accuracy.cpu()), 2), color = 'r', linestyle = '-')\n",
    "\n",
    "    max_acc_train = float(max(plot_dict['train_acc_list']))\n",
    "    max_acc_train_idx = int(np.argmax(plot_dict['train_acc_list']))\n",
    "    axes[kfold_counter, 1].annotate(f'Highest train Acc: {round(max_acc_train, 2)}', \n",
    "            xy=(max_acc_train_idx, max_acc_train),\n",
    "            xytext=(max_acc_train_idx, max_acc_train -0.15),  # Adjust text position\n",
    "            arrowprops=dict(facecolor='red', arrowstyle='->'))\n",
    "    \n",
    "    max_valid_acc = float(max(plot_dict['val_acc_list']))\n",
    "    max_valid_acc_idx = int(np.argmax(plot_dict['val_acc_list']))\n",
    "    axes[kfold_counter, 1].annotate(f'Highest Val Acc: {round(max_valid_acc, 2)}', \n",
    "            xy=(max_valid_acc_idx, max_valid_acc),\n",
    "            xytext=(max_valid_acc_idx, max_valid_acc -0.15),  # Adjust text position\n",
    "            arrowprops=dict(facecolor='red', arrowstyle='->'))\n",
    "\n",
    "    axes[kfold_counter, 1].set_xlabel('Epochs')\n",
    "    axes[kfold_counter, 1].set_ylabel('Accuracy')\n",
    "    axes[kfold_counter, 1].legend()\n",
    "\n",
    "    axes[kfold_counter, 2].imshow(cm, cmap='Blues')\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            axes[kfold_counter, 2].text(j, i, f'{cm[i, j]}', ha='center', va='center', color='black', fontsize=12)\n",
    "    axes[kfold_counter, 2].set_xticks(np.arange(len(cm)))\n",
    "    axes[kfold_counter, 2].set_yticks(np.arange(len(cm)))\n",
    "    axes[kfold_counter, 2].set_xlabel('Predicted label')\n",
    "    axes[kfold_counter, 2].set_ylabel('True label')\n",
    "    axes[kfold_counter, 2].set_title('Confusion Matrix Heatmap')\n",
    "    axes[kfold_counter, 2].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_reset_train_valid_masks (graphs_dataset):\n",
    "    graphs_list = []\n",
    "    train_mask_list = []\n",
    "    val_mask_list = []\n",
    "    plant_tags_list = []\n",
    "    label_list = []\n",
    "    for batchdata in graphs_dataset:\n",
    "        batchdata_list = batchdata.to_data_list()\n",
    "        for data in batchdata_list:\n",
    "            graphs_list.append(data)\n",
    "            for i in data.train_mask:\n",
    "                train_mask_list.append(int(i))\n",
    "            for i in data.val_mask:\n",
    "                val_mask_list.append(int(i))\n",
    "            for i in data.plant_tags:\n",
    "                plant_tags_list.append(int(i))\n",
    "            for i in data.y:\n",
    "                label_list.append(int(i))\n",
    "    \n",
    "    train_mask_array = np.array(train_mask_list)\n",
    "    val_mask_array = np.array(val_mask_list)\n",
    "    plant_tags_array = np.array(plant_tags_list)\n",
    "    label_array = np.array(label_list)\n",
    "\n",
    "    combined_mask = np.logical_or(train_mask_array, val_mask_array)\n",
    "    num_ones = np.sum(combined_mask)\n",
    "    num_zeros = len(combined_mask) - num_ones\n",
    "\n",
    "    filtered_plant_tags = plant_tags_array[combined_mask]\n",
    "    filtered_labels = label_array[combined_mask]\n",
    "\n",
    "    return filtered_plant_tags, filtered_labels, graphs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_kfold_mask(filtered_plant_tags, train_idx, valid_idx, graphs_list):\n",
    "    train_tags = [filtered_plant_tags[i] for i in train_idx]\n",
    "    valid_tags = [filtered_plant_tags[i] for i in valid_idx]\n",
    "\n",
    "    #assigning new masks\n",
    "    for graph in graphs_list:\n",
    "        #create new masks array\n",
    "        new_train_mask = np.isin(graph.plant_tags, train_tags)\n",
    "        new_val_mask = np.isin(graph.plant_tags, valid_tags)\n",
    "        new_test_mask = np.logical_and(~new_train_mask, ~new_val_mask)\n",
    "\n",
    "        graph.train_maks = torch.Tensor(new_train_mask)\n",
    "        graph.val_maks = torch.Tensor(new_val_mask)\n",
    "        graph.test_maks = torch.Tensor(new_test_mask)\n",
    "    \n",
    "    return graphs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_edges_for_graphs(graphs_list, p, force_undirected=True):\n",
    "    \n",
    "    for data in graphs_list:\n",
    "\n",
    "        new_edge_index, edge_mask = dropout_edge(data.edge_index, p=p, force_undirected=force_undirected)\n",
    "        data.edge_index = new_edge_index\n",
    "    \n",
    "    return graphs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_features_for_graphs(graphs_list, p):\n",
    "    \n",
    "    for data in graphs_list:\n",
    "\n",
    "        x, feat_mask = mask_feature(data.x, p=p, mode ='all', fill_value=0)\n",
    "        data.x = x\n",
    "    \n",
    "    return graphs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_crop_name(input_string):\n",
    "     # Define the possible crop names\n",
    "    onion_list = [\"oignon\", \"onion\", \"oignons\", \"onions\"]\n",
    "    carrot_list = [\"carrot\", \"carrots\", \"carotte\", \"carottes\"]\n",
    "    lettuce_list = [\"lettuce\", \"letucces\", \"laitue\", 'laitues']\n",
    "\n",
    "    # Track matches for each list\n",
    "    matches = {\n",
    "        \"onion\": any(word in input_string for word in onion_list),\n",
    "        \"carrot\": any(word in input_string for word in carrot_list),\n",
    "        \"lettuce\": any(word in input_string for word in lettuce_list)\n",
    "    }\n",
    "\n",
    "    # Count the number of lists with matches\n",
    "    match_count = sum(matches.values())\n",
    "\n",
    "    if match_count == 1:\n",
    "        for key, matched in matches.items():\n",
    "            if matched:\n",
    "                return f\"{key}\"\n",
    "    elif match_count > 1:\n",
    "        return \"Unknown\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created folder ../../Output/Trained_models_masking/experiment_27\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'models_result_replication/onion_base'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#create ouput path, save analysis config, and create result_df\u001b[39;00m\n\u001b[0;32m     28\u001b[0m trained_models_path, result_df \u001b[38;5;241m=\u001b[39m build_result_dir_and_files(config_dict, param_grid)\n\u001b[1;32m---> 30\u001b[0m dataset_count \u001b[38;5;241m=\u001b[39m \u001b[43mcount_unique_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m param_combi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mparam_grid\u001b[38;5;241m.\u001b[39mvalues())))\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mdataset_count\u001b[38;5;241m*\u001b[39mconfig_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkfold\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39mparam_combi\u001b[38;5;241m*\u001b[39mconfig_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloops\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m pbar: \n",
      "Cell \u001b[1;32mIn [10], line 4\u001b[0m, in \u001b[0;36mcount_unique_datasets\u001b[1;34m(project)\u001b[0m\n\u001b[0;32m      2\u001b[0m dataset_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m project:\n\u001b[1;32m----> 4\u001b[0m     folder_list \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     dataset_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(folder_list)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset_count\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'models_result_replication/onion_base'"
     ]
    }
   ],
   "source": [
    "analysis_params= {\n",
    "    \"project\": ['graph_datasets/onion/onion_base'],\n",
    "    \"classes\": 2,\n",
    "    \"loops\": 5,\n",
    "    \"kfold\": 5,\n",
    "    \"start_seed\": 0,\n",
    "    \"restart\": None\n",
    "}\n",
    "\n",
    "#store search parameters here\n",
    "param_grid = { \n",
    "    'batch_size' : [8, 16], #keep batch size first\n",
    "    'max_epochs': [2000],\n",
    "    'num_layers': [3, 4, 5],\n",
    "    'hidden_channels': [32, 34],\n",
    "    'optimizer_type': ['Adam'], \n",
    "    'learning_rate': [0.00001],\n",
    "    'dropout_rate': [0, 0.1],\n",
    "    'fc_layer_channels' : [0]\n",
    "    }\n",
    "\n",
    "# create result path, \n",
    "config_dict = generate_analysis_config(**analysis_params, **param_grid )\n",
    "\n",
    "param_dict = dict()\n",
    "param_folder= None\n",
    "\n",
    "#create ouput path, save analysis config, and create result_df\n",
    "trained_models_path, result_df = build_result_dir_and_files(config_dict, param_grid)\n",
    "\n",
    "dataset_count = count_unique_datasets(config_dict[\"project\"])\n",
    "param_combi = len(list(itertools.product(*param_grid.values())))\n",
    "\n",
    "with tqdm(total=dataset_count*config_dict[\"kfold\"]*param_combi*config_dict[\"loops\"]) as pbar: \n",
    "    for loop in range(config_dict[\"loops\"]):\n",
    "        print(f'LOOP {loop+1}/{config_dict[\"loops\"]} STARTING')\n",
    "\n",
    "        for datasets_dir in config_dict[\"project\"]:\n",
    "            dataset_list = os.listdir(datasets_dir)\n",
    "            for dataset_folder in dataset_list:\n",
    "                dataset = dataset_folder.split('_', 1)[0]\n",
    "                aug = dataset_folder.split('_', 1)[1]\n",
    "                graphs_dataset = torch.load(f'{datasets_dir}/{dataset_folder}/graphs_dataset.pth')\n",
    "\n",
    "                param_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "                for param in param_combinations:\n",
    "                    active_row = False\n",
    "                    param_folder = f'params_{param}' #loop\n",
    "\n",
    "                    json_path = pathlib.Path(f'{trained_models_path}/{datasets_dir}/{dataset_folder}/{param_folder}')\n",
    "\n",
    "                    # Check if the file exists and is a file\n",
    "                    if json_path.exists() and json_path.is_file():\n",
    "                        pass\n",
    "                    else:\n",
    "                        #extract the first graph to find out the num of features in the graph dataset\n",
    "                        for batch in graphs_dataset:\n",
    "                            first_batch = batch\n",
    "                            num_features = first_batch[0].num_features\n",
    "                            break\n",
    "\n",
    "                        run_param_dict = dict(map(lambda i,j : (i,j) , param_grid.keys(), param))\n",
    "                        run_param_dict['crop'] = extract_crop_name(str(datasets_dir))\n",
    "                        run_param_dict['dataset'] = dataset\n",
    "                        run_param_dict['augmentation'] = aug\n",
    "                        run_param_dict['num_features'] = num_features\n",
    "                        json_path.mkdir(parents=True, exist_ok=True)\n",
    "                        create_analysis_config_file(json_path, 'model_config.json', run_param_dict)\n",
    "                        \n",
    "\n",
    "\n",
    "                    plt.close() #avoids opening multiple figures for nothing when restarting an analysis\n",
    "                    fig, axes = plt.subplots(nrows = config_dict[\"kfold\"], ncols =3, figsize=(15, 3*config_dict[\"kfold\"]))\n",
    "                    plt.suptitle(f'dataset: {dataset} \\n augmentation: {aug} \\n params: {param}')\n",
    "\n",
    "                    #####################\n",
    "                    ### kfold start  ####\n",
    "                    #####################\n",
    "                    kfold_counter = -1\n",
    "                    seed = config_dict[\"start_seed\"]\n",
    "            \n",
    "                    filtered_plant_tags, filtered_labels, complete_graphs_list = kfold_reset_train_valid_masks(graphs_dataset)\n",
    "\n",
    "                    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state= seed)\n",
    "                    seed += 1\n",
    "\n",
    "                    for train_idx, valid_idx in skf.split(filtered_plant_tags, filtered_labels):\n",
    "\n",
    "                        graphs_list = shift_kfold_mask(filtered_plant_tags, train_idx, valid_idx, complete_graphs_list)\n",
    "                        modified_graphs_list = copy.deepcopy(graphs_list)\n",
    "\n",
    "                        graphs_dataset_kfold = DataLoader(modified_graphs_list, batch_size=param[0], shuffle=True) #pin_memory=True\n",
    "                        kfold_counter += 1\n",
    "\n",
    "                        new_row = {\n",
    "                        'crop': datasets_dir, \n",
    "                        'dataset': dataset, \n",
    "                        'augmentation': aug,\n",
    "                        'loop': loop,\n",
    "                        'run_number': kfold_counter,\n",
    "                        'parameters': param\n",
    "                        }\n",
    "\n",
    "                        #print(f'{row_exists(result_df, new_row)}')\n",
    "                        if not row_exists(result_df, new_row):\n",
    "                            active_row = True\n",
    "                            pbar.set_description(f\"Processing {dataset_folder} - run {kfold_counter}\")\n",
    "                            best_accuracy, params, model_state_dict, cm, roc_auc, plot_dict, cumul_variance = model_training(graphs_dataset = graphs_dataset_kfold,\n",
    "                                                                                    num_classes=config_dict[\"classes\"], \n",
    "                                                                                    params=param)\n",
    "\n",
    "                            params_values_dict = dict(map(lambda i,j : (i,j) , param_grid.keys(), param))\n",
    "                            model_number = (loop*config_dict['kfold']) + kfold_counter\n",
    "                            torch.save(model_state_dict, f'{trained_models_path}/{datasets_dir}/{dataset_folder}/{param_folder}/{dataset}_{model_number}.pt')\n",
    "\n",
    "                            if (cm[0, 0] + cm[0, 1]) == 0:\n",
    "                                precision = 0\n",
    "                            else:\n",
    "                                precision = float(cm[0, 0]/(cm[0, 0] + cm[0, 1])) # TP/(TP + FP)\n",
    "                            \n",
    "                            if (cm[0, 0] + cm[1, 0]) == 0:\n",
    "                                rappel = 0\n",
    "                            else:\n",
    "                                recall = float(cm[0, 0]/(cm[0, 0] + cm[1, 0])) # TP/(TP+FN)\n",
    "                            \n",
    "                            if precision > 0 and recall > 0:\n",
    "                                f1_score = float((2 * precision * recall)/(precision + recall))\n",
    "                            else:\n",
    "                                f1_score = 0\n",
    "\n",
    "                            result_dict= {\n",
    "                                'accuracy': float(best_accuracy*100), \n",
    "                                'TP': cm[0, 0], \n",
    "                                'FP':cm[0, 1], \n",
    "                                'FN':cm[1, 0], \n",
    "                                'TN':cm[1, 1], \n",
    "                                'ROC-AUC': float(roc_auc),\n",
    "                                'precision': precision,\n",
    "                                'recall': recall,\n",
    "                                'F1-score': f1_score,\n",
    "                                'epochs': len(plot_dict['train_losses_list']),\n",
    "                                'cumul_variance' : cumul_variance\n",
    "                            }\n",
    "\n",
    "                            #merge 3 dicts to complete the row in the result_df\n",
    "                            new_row.update(result_dict)\n",
    "                            new_row.update(params_values_dict)\n",
    "\n",
    "                            result_df = add_row(result_df, new_row)\n",
    "                            result_df.to_csv(f'{trained_models_path}/summary.csv')\n",
    "\n",
    "\n",
    "                            # Add the new plots to the existing subplots\n",
    "                            add_subplot(axes, kfold_counter, plot_dict, best_accuracy, cm)\n",
    "\n",
    "                            # param_folder only None if retracing already done runs\n",
    "                            if param_folder is not None:\n",
    "                                plt.savefig(f'{trained_models_path}/{datasets_dir}/{dataset_folder}/{param_folder}{dataset_folder}_loop_{loop}_run{kfold_counter}.svg')\n",
    "                            \n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                    # param_folder only None if retracing already done runs\n",
    "                    if param_folder is not None and active_row:   \n",
    "                        plt.savefig(f'{trained_models_path}/{datasets_dir}/{dataset_folder}/{param_folder}/{dataset_folder}_loop_{loop}_summary.svg')\n",
    "                        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
